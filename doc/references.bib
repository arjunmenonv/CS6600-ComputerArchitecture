
@book{shen_modern_2013,
	address = {Long Grove, Illinois},
	edition = {Reissued},
	title = {Modern processor design: fundamentals of superscalar processors},
	isbn = {978-1-4786-0783-0},
	shorttitle = {Modern processor design},
	language = {eng},
	publisher = {Waveland Press},
	author = {Shen, John Paul and Lipasti, Mikko H.},
	year = {2013},
}

@article{maass_ecotlb_2020,
	title = {{ecoTLB}: {Eventually} {Consistent} {TLBs}},
	volume = {17},
	issn = {1544-3566},
	shorttitle = {{\textless}span class="smallcaps {smallerCapital}"{\textgreater}{ECO}{\textless}/span{\textgreater}{TLB}},
	url = {https://doi.org/10.1145/3409454},
	doi = {10.1145/3409454},
	abstract = {We propose ecoTLB—software-based eventual translation lookaside buffer (TLB) coherence—which eliminates the overhead of the synchronous TLB shootdown mechanism in operating systems that use address space identifiers (ASIDs). With an eventual TLB coherence, ecoTLB improves the performance of free and page swap operations by removing the inter-processor interrupt (IPI) overheads incurred to invalidate TLB entries. We show that the TLB shootdown has implications for page swapping in particular in emerging, disaggregated data centers and demonstrate that ecoTLB can improve both the performance and the specific swapping policy decisions using ecoTLB’s asynchronous mechanism. We demonstrate that ecoTLB improves the performance of real-world applications, such as Memcached and Make, that perform page swapping using Infiniswap, a solution for next generation data centers that use disaggregated memory, by up to 17.2\%. Moreover, ecoTLB improves the 99th percentile tail latency of Memcached by up to 70.8\% due to its asynchronous scheme and improved policy decisions. Furthermore, we show that recent features to improve security in the Linux kernel, like kernel page table isolation (KPTI), can result in significant performance overheads on architectures without support for specific instructions to clear single entries in tagged TLBs, falling back to full TLB flushes. In this scenario, ecoTLB is able to recover the performance lost for supporting KPTI due to its asynchronous shootdown scheme and its support for tagged TLBs. Finally, we demonstrate that ecoTLB improves the performance of free operations by up to 59.1\% on a 120-core machine and improves the performance of Apache on a 16-core machine by up to 13.7\% compared to baseline Linux, and by up to 48.2\% compared to ABIS, a recent state-of-the-art research prototype that reduces the number of IPIs.},
	number = {4},
	urldate = {2021-10-12},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Maass, Steffen and Kumar, Mohan Kumar and Kim, Taesoo and Krishna, Tushar and Bhattacharjee, Abhishek},
	month = sep,
	year = {2020},
	keywords = {TLB, asynchrony, translation coherence},
	pages = {27:1--27:24},
}

@inproceedings{bhattacharjee_inter-core_2010,
	address = {New York, NY, USA},
	series = {{ASPLOS} {XV}},
	title = {Inter-core cooperative {TLB} for chip multiprocessors},
	isbn = {978-1-60558-839-1},
	url = {https://doi.org/10.1145/1736020.1736060},
	doi = {10.1145/1736020.1736060},
	abstract = {Translation Lookaside Buffers (TLBs) are commonly employed in modern processor designs and have considerable impact on overall system performance. A number of past works have studied TLB designs to lower access times and miss rates, specifically for uniprocessors. With the growing dominance of chip multiprocessors (CMPs), it is necessary to examine TLB performance in the context of parallel workloads. This work is the first to present TLB prefetchers that exploit commonality in TLB miss patterns across cores in CMPs. We propose and evaluate two Inter-Core Cooperative (ICC) TLB prefetching mechanisms, assessing their effectiveness at eliminating TLB misses both individually and together. Our results show these approaches require at most modest hardware and can collectively eliminate 19\% to 90\% of data TLB (D-TLB) misses across the surveyed parallel workloads. We also compare performance improvements across a range of hardware and software implementation possibilities. We find that while a fully-hardware implementation results in average performance improvements of 8-46\% for a range of TLB sizes, a hardware/software approach yields improvements of 4-32\%. Overall, our work shows that TLB prefetchers exploiting inter-core correlations can effectively eliminate TLB misses.},
	urldate = {2021-10-07},
	booktitle = {Proceedings of the fifteenth {International} {Conference} on {Architectural} support for programming languages and operating systems},
	publisher = {Association for Computing Machinery},
	author = {Bhattacharjee, Abhishek and Martonosi, Margaret},
	month = mar,
	year = {2010},
	keywords = {parallelism, prefetching, translation lookaside buffer},
	pages = {359--370},
}

@inproceedings{baruah_valkyrie_2020,
	address = {New York, NY, USA},
	series = {{PACT} '20},
	title = {Valkyrie: {Leveraging} {Inter}-{TLB} {Locality} to {Enhance} {GPU} {Performance}},
	isbn = {978-1-4503-8075-1},
	shorttitle = {Valkyrie},
	url = {https://doi.org/10.1145/3410463.3414639},
	doi = {10.1145/3410463.3414639},
	abstract = {Programming on a GPU has been made considerably easier with the introduction of Virtual Memory features, which support common pointer-based semantics between the CPU and the GPU. However, supporting virtual memory on a GPU comes with some additional costs and overhead, with the largest being from the support for address translation. The fact that a massive number of threads run concurrently on a GPU means that the translation lookaside buffers (TLBs) are oversubscribed most of the time. Our investigation into a diverse set of GPU workloads shows that TLB misses can be extremely high (up to 99\%), which inevitably leads to significant performance degradation due to long-latency page-table walks. Our profiling of TLB-sensitive workloads reveals a high degree of page sharing across the different cores of a GPU. In many applications, a page can be accessed in temporal proximity by multiple cores, following similar memory access patterns. To support the inherent sharing present in GPU workloads, we propose Valkyrie, an integrated cooperative TLB prefetching mechanism and an inter L1-TLB probing scheme that can efficiently reduce TLB bottlenecks in GPUs. Our evaluation using a diverse set of GPU workloads reveals that Valkyrie is able to achieve an average speedup of 1.95x, while adding modest hardware overhead.},
	urldate = {2021-10-06},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	publisher = {Association for Computing Machinery},
	author = {Baruah, Trinayan and Sun, Yifan and Mojumder, Saiful A. and Abellán, José L. and Ukidave, Yash and Joshi, Ajay and Rubin, Norman and Kim, John and Kaeli, David},
	month = sep,
	year = {2020},
	keywords = {gpu, tlb design, tlb prefetching, tlb probing, virtual memory},
	pages = {455--466},
}

@phdthesis{bienia_benchmarking_2011,
	address = {USA},
	title = {Benchmarking modern multiprocessors},
	abstract = {Benchmarking has become one of the most important methods for quantitative performance evaluation of processor and computer system designs. Benchmarking of modern multiprocessors such as chip multiprocessors is challenging because of their application domain, scalability and parallelism requirements. In my thesis, I have developed a methodology to design effective benchmark suites and demonstrated its effectiveness by developing and deploying a benchmark suite for evaluating multiprocessors. More specifically, this thesis includes several contributions. First, the thesis shows that a new benchmark suite for multiprocessors is needed because the behavior of modern parallel programs is significantly different from those represented by SPLASH-2, the most popular parallel benchmark suite developed over ten years ago. Second, the thesis quantitatively describes the requirements and characteristics of a set of multithreaded programs and their underlying technology trends. Third, the thesis presents a systematic approach to scale and select benchmark inputs with the goal of optimizing benchmarking accuracy subject to constrained execution or simulation time. Finally, the thesis describes a parallel benchmark suite called PARSEC for evaluating modern shared-memory multiprocessors. Since its initial release, PARSEC has been adopted by many architecture groups in both research and industry.},
	school = {Princeton University},
	author = {Bienia, Christian},
	year = {2011},
	note = {ISBN: 9781124491868},
}

@inproceedings{bhattacharjee_shared_2011,
	title = {Shared last-level {TLBs} for chip multiprocessors},
	doi = {10.1109/HPCA.2011.5749717},
	abstract = {Translation Lookaside Buffers (TLBs) are critical to processor performance. Much past research has addressed uniprocessor TLBs, lowering access times and miss rates. However, as chip multiprocessors (CMPs) become ubiquitous, TLB design must be re-evaluated. This paper is the first to propose and evaluate shared last-level (SLL) TLBs as an alternative to the commercial norm of private, per-core L2 TLBs. SLL TLBs eliminate 7-79\% of system-wide misses for parallel workloads. This is an average of 27\% better than conventional private, per-core L2 TLBs, translating to notable runtime gains. SLL TLBs also provide benefits comparable to recently-proposed Inter-Core Cooperative (ICC) TLB prefetchers, but with considerably simpler hardware. Furthermore, unlike these prefetchers, SLL TLBs can aid sequential applications, eliminating 35-95\% of the TLB misses for various multiprogrammed combinations of sequential applications. This corresponds to a 21\% average increase in TLB miss eliminations compared to private, per-core L2 TLBs. Because of their benefits for parallel and sequential applications, and their readily-implementable hardware, SLL TLBs hold great promise for CMPs.},
	booktitle = {2011 {IEEE} 17th {International} {Symposium} on {High} {Performance} {Computer} {Architecture}},
	author = {Bhattacharjee, Abhishek and Lustig, Daniel and Martonosi, Margaret},
	month = feb,
	year = {2011},
	note = {ISSN: 2378-203X},
	keywords = {Benchmark testing, Hardware, Iterative closest point algorithm, Lead, Prefetching, Runtime, System performance},
	pages = {62--63},
}

@inproceedings{bharadwaj_scalable_2018,
	address = {Fukuoka, Japan},
	series = {{MICRO}-51},
	title = {Scalable distributed last-level {TLBs} using low-latency interconnects},
	isbn = {978-1-5386-6240-3},
	url = {https://doi.org/10.1109/MICRO.2018.00030},
	doi = {10.1109/MICRO.2018.00030},
	abstract = {Recent studies have shown the potential of last-level TLBs shared by multiple cores in tackling memory translation performance challenges posed by "big data" workloads. A key stumbling block hindering their effectiveness, however, is their high access time. We present a design methodology to reduce these high access times so as to realize high-performance and scalable shared L2 TLBs. As a first step, we study the benefits of replacing monolithic shared TLBs with a distributed set of small TLB slices. While this approach does reduce TLB lookup latency, it increases interconnect delays in accessing remote slices. Therefore, as a second step, we devise a lightweight single-cycle interconnect among the TLB slices by tailoring wires and switches to the unique communication characteristics of memory translation requests and responses. Our approach, which we dub Nocstar ({\textless}u{\textgreater}NOC{\textless}/u{\textgreater}s for {\textless}u{\textgreater}s{\textless}/u{\textgreater}calable {\textless}u{\textgreater}T{\textless}/u{\textgreater}LB {\textless}u{\textgreater}a{\textless}/u{\textgreater}rchitecture), combines the high hit rates of shared TLBs with low access times of private L2 TLBs, enabling significant system performance benefits.},
	urldate = {2021-10-01},
	booktitle = {Proceedings of the 51st {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {IEEE Press},
	author = {Bharadwaj, Srikant and Cox, Guilherme and Krishna, Tushar and Bhattacharjee, Abhishek},
	month = oct,
	year = {2018},
	keywords = {TLB, caches, network-on-chip, virtual memory},
	pages = {271--284},
}

@article{kumar_heterogeneous_2005,
	title = {Heterogeneous chip multiprocessors},
	volume = {38},
	issn = {1558-0814},
	doi = {10.1109/MC.2005.379},
	abstract = {Heterogeneous (or asymmetric) chip multiprocessors present unique opportunities for improving system throughput, reducing processor power, and mitigating Amdahl's law. On-chip heterogeneity allow the processor to better match execution resources to each application's needs and to address a much wider spectrum of system loads - from low to high thread parallelism - with high efficiency.},
	number = {11},
	journal = {Computer},
	author = {Kumar, R. and Tullsen, D.M. and Jouppi, N.P. and Ranganathan, P.},
	month = nov,
	year = {2005},
	note = {Conference Name: Computer},
	keywords = {CMP, Chip multiprocessors, Clocks, Costs, Frequency, Heterogeneity, IEEE news, Microprocessors, Moore's Law, Multicore microprocessors, Multicore processing, Multiprocessors, Portable computers, Power-aware computing, Sun, System architectures, Throughput},
	pages = {32--38},
}

@article{sankaralingam_detailed_2013,
	title = {A {Detailed} {Analysis} of {Contemporary} {ARM} and x86 {Architectures}},
	url = {https://minds.wisconsin.edu/handle/1793/64923},
	abstract = {RISC vs. CISC wars raged in the 1980s when chip area and processor design complexity were the primary constraints and desktops and servers exclusively dominated the computing landscape. Today, energy and power are the primary design constraints and the computing landscape is significantly different: growth in tablets and smartphones running ARM (a RISC ISA) is surpassing that of desktops and laptops running x86 (a CISC ISA). Further, the traditionally low-power ARM ISA is entering the high-performance server market, while the traditionally high-performance x86 ISA is entering the mobile low-power device market. Thus, the question of whether ISA plays an intrinsic role in performance or energy efficiency is becoming important, and we seek to answer this question through a detailed measurement based study on real hardware running real applications. We analyze measurements on the ARM Cortex-A8 and Cortex-A9 and Intel Atom and Sandybridge i7 microprocessors over workloads spanning mobile, desktop, and server computing. Our methodical investigation demonstrates the role of ISA in modern microprocessors? performance and energy efficiency. We find that ARM and x86 processors are simply engineering design points optimized for different levels of performance, and there is nothing fundamentally more energy efficient in one ISA class or the other. The ISA being RISC or CISC seems irrelevant.},
	language = {en},
	urldate = {2021-10-01},
	author = {Sankaralingam, Karthikeyan and Menon, Jaikrishnan and Blem, Emily},
	month = feb,
	year = {2013},
	note = {Accepted: 2013-02-28T20:44:24Z},
}

@article{nayfeh_single-chip_1997,
	title = {A single-chip multiprocessor},
	volume = {30},
	issn = {1558-0814},
	doi = {10.1109/2.612253},
	abstract = {Presents the case for billion-transistor processor architectures that will consist of chip multiprocessors (CMPs): multiple (four to 16) simple, fast processors on one chip. In their proposal, each processor is tightly coupled to a small, fast, level-one cache, and all processors share a larger level-two cache. The processors may collaborate on a parallel job or run independent tasks (as in the SMT proposal). The CMP architecture lends itself to simpler design, faster validation, cleaner functional partitioning, and higher theoretical peak performance. However for this architecture to realize its performance potential, either programmers or compilers will have to make code explicitly parallel. Old ISAs will be incompatible with this architecture (although they could run slowly on one of the small processors).},
	number = {9},
	journal = {Computer},
	author = {Nayfeh, B.A. and Olukotun, K.},
	month = sep,
	year = {1997},
	note = {Conference Name: Computer},
	keywords = {Application software, Circuits, Computer architecture, Costs, Delay effects, Hardware, Microarchitecture, Parallel processing, Surface-mount technology},
	pages = {79--85},
}

@article{hill_amdahls_2008,
	title = {Amdahl's {Law} in the {Multicore} {Era}},
	volume = {41},
	issn = {1558-0814},
	doi = {10.1109/MC.2008.209},
	abstract = {Augmenting Amdahl's law with a corollary for multicore hardware makes it relevant to future generations of chips with multiple processor cores. Obtaining optimal multicore performance will require further research in both extracting more parallelism and making sequential cores faster.},
	number = {7},
	journal = {Computer},
	author = {Hill, Mark D. and Marty, Michael R.},
	month = jul,
	year = {2008},
	note = {Conference Name: Computer},
	keywords = {Amdahl's law, Computer architecture, Costs, Energy management, Equations, Hardware, Multicore processing, Multiprocessor interconnection networks, Parallel processing, Pipelines, Roads, chip multiprocessors (CMPs), multicore chips},
	pages = {33--38},
}

@inproceedings{tullsen_simultaneous_1995,
	title = {Simultaneous multithreading: {Maximizing} on-chip parallelism},
	shorttitle = {Simultaneous multithreading},
	abstract = {This paper examines simultaneous multithreading, a technique permitting several independent threads to issue instructions to a superscalar's multiple functional units in a single cycle. We present several models of simultaneous multithreading and compare them with alternative organizations: a wide superscalar, a fine-grain multithreaded processor, and single-chip, multiple-issue multiprocessing architectures. Our results show that both (single-threaded) superscalar and fine-grain multithreaded architectures are limited in their ability to utilize the resources of a wide-issue processor. Simultaneous multithreading has the potential to achieve 4 times the throughput of a superscalar, and double that of fine-grain multi-threading. We evaluate several cache configurations made possible by this type of organization and evaluate tradeoffs between them. We also show that simultaneous multithreading is an attractive alternative to single-chip multiprocessors; simultaneous multithreaded processors with a variety of organizations outperform corresponding conventional multiprocessors with similar execution resources. While simultaneous multithreading has excellent potential to increase processor utilization, it can add substantial complexity to the design. We examine many of these complexities and evaluate alternative organizations in the design space.},
	booktitle = {Proceedings 22nd {Annual} {International} {Symposium} on {Computer} {Architecture}},
	author = {Tullsen, D.M. and Eggers, S.J. and Levy, H.M.},
	month = jun,
	year = {1995},
	note = {ISSN: 1063-6897},
	keywords = {Computer science, Delay, Modems, Multithreading, Parallel processing, Permission, Samarium, Switches, Throughput, Yarn},
	pages = {392--403},
}

@inproceedings{sultana_intelligent_2020,
	address = {New York, NY, USA},
	series = {{PACT} '20},
	title = {Intelligent {Data} {Placement} on {Discrete} {GPU} {Nodes} with {Unified} {Memory}},
	isbn = {978-1-4503-8075-1},
	url = {https://doi.org/10.1145/3410463.3414651},
	doi = {10.1145/3410463.3414651},
	abstract = {With increasing heterogeneity, the importance of data organization within a compute node has grown immensely. Recently, industry vendors have introduced technology that can present a unified shared address space for multiple physical pools of memory. In this paper, we leverage unified memory technology and characterize the performance trade-offs of host and device placement across a range of hybrid application design patterns. We perform a Roofline analysis to establish fundamental performance bounds in collaborative applications and then develop an analytical model that makes profitable placement decisions at the individual data structure level. We integrate the placement model into a runtime system and enable transparent data placement in CUDA/C++ applications. Preliminary experiments yield the following results: (i) placement policies have significant performance impact across hybrid application design paradigms (ii) placement decisions are impacted by the sparsity of data access, page re-migration, amount of latency hiding opportunities and design specific attributes such as the number of pipeline stages, and (iii) intelligent data placement can improve node performance by up to 5x on applications with sparse access patterns.},
	urldate = {2021-09-30},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	publisher = {Association for Computing Machinery},
	author = {Sultana, Tanzima and Allen, Blake and Qasem, Apan},
	month = sep,
	year = {2020},
	keywords = {data placement, heterogeneous computing, performance modeling, roofline analysis, unified memory},
	pages = {139--151},
}

@article{kandiraju_characterizing_2002,
	title = {Characterizing the d-{TLB} behavior of {SPEC} {CPU2000} benchmarks},
	volume = {30},
	issn = {0163-5999},
	url = {https://doi.org/10.1145/511399.511351},
	doi = {10.1145/511399.511351},
	abstract = {Despite the numerous optimization and evaluation studies that have been conducted with TLBs over the years, there is still a deficiency in an indepth understanding of TLB characteristics from an application angle. This paper presents a detailed characterization study of the TLB behavior of the SPEC CPU2000 benchmark suite. The contributions of this work are in identifying important application characteristics for TLB studies, quantifying the SPEC2000 application behavior for these characteristics, as well as making pronouncements and suggestions for future research based on these results.Around one-fourth of the SPEC2000 applications (ammp, apsi, galgel, lucas, mcf, twolf and vpr) have significant TLB missrates. Both capacity and associativity are influencing factors on miss-rates, though they do not necessarily go hand-in-hand. Multi-level TLBs are definitely useful for these applications in cutting down access times without significant miss rate degradation. Superpaging to combine TLB entries may not be rewarding for many of these applications. Software management of TLBs in terms of determining what entries to prefetch, what entries to replace, and what entries to pin has a lot of potential to cut down miss rates considerably. Specifically, the potential benefits of prefetching TLB entries is examined, and Distance Prefetching is shown to give good prediction accuracy for these applications.},
	number = {1},
	urldate = {2021-09-29},
	journal = {ACM SIGMETRICS Performance Evaluation Review},
	author = {Kandiraju, Gokul B. and Sivasubramaniam, Anand},
	month = jun,
	year = {2002},
	pages = {129--139},
}

@article{stenstrom_survey_1990,
	title = {A survey of cache coherence schemes for multiprocessors},
	volume = {23},
	issn = {1558-0814},
	doi = {10.1109/2.55497},
	abstract = {Schemes for cache coherence that exhibit various degrees of hardware complexity, ranging from protocols that maintain coherence in hardware, to software policies that prevent the existence of copies of shared, writable data, are surveyed. Some examples of the use of shared data are examined. These examples help point out a number of performance issues. Hardware protocols are considered. It is seen that consistency can be maintained efficiently, although in some cases with considerable hardware complexity, especially for multiprocessors with many processors. Software schemes are investigated as an alternative capable of reducing the hardware cost.{\textless}{\textgreater}},
	number = {6},
	journal = {Computer},
	author = {Stenstrom, P.},
	month = jun,
	year = {1990},
	note = {Conference Name: Computer},
	keywords = {Costs, Hardware, Protocols, Software maintenance},
	pages = {12--24},
}

@book{jacob_memory_2008,
	address = {Burlington, MA},
	title = {Memory systems: {Cache}, {DRAM}, {Disk}},
	isbn = {978-0-12-379751-3},
	shorttitle = {Memory systems},
	publisher = {Morgan Kaufmann Publishers},
	author = {Jacob, Bruce and Ng, Spencer W. and Wang, David T.},
	year = {2008},
	note = {OCLC: ocn154760293},
	keywords = {Computer storage devices},
}

@inproceedings{chaudhuri_zero_2021,
	title = {Zero {Inclusion} {Victim}: {Isolating} {Core} {Caches} from {Inclusive} {Last}-level {Cache} {Evictions}},
	shorttitle = {Zero {Inclusion} {Victim}},
	url = {https://ieeexplore.ieee.org/document/9499802},
	doi = {10.1109/ISCA52012.2021.00015},
	abstract = {The most widely used last-level cache (LLC) architecture in the microprocessors has been the inclusive LLC design. The popularity of the inclusive design stems from the bandwidth optimization and simplification it offers to the implementation of the cache coherence protocols. However, inclusive LLCs have always been associated with the curse of inclusion victims. An inclusion victim is a block that must be forcefully replaced from the inner levels of the cache hierarchy when the copy of the block is replaced from the inclusive LLC. This tight coupling between the LLC victims and the inner-level cache contents leads to three major drawbacks. First, live inclusion victims can lead to severe performance degradation depending on the LLC replacement policies. Second, a process can victimize the blocks of another process in an LLC shared by multiple cores and this can be exploited to leak information through well-known eviction-based timing side-channels. An inclusive LLC makes these channels much less noisy due to the presence of inclusion victims which allow the malicious processes to control the contents of the percore private caches through LLC evictions. Third, to reduce the impact of the aforementioned two drawbacks, the inner-level caches, particularly the mid-level cache in a three-level inclusive cache hierarchy, must be kept small even if a larger mid-level cache could have been beneficial in the absence of inclusion victims.We observe that inclusion victims are not fundamental to the inclusion property, but arise due to the way the contents of an inclusive LLC are managed. Motivated by this observation, we introduce a fundamentally new inclusive LLC design named the Zero Inclusion Victim (ZIV) LLC that guarantees freedom from inclusion victims while retaining all advantages of an inclusive LLC. This is the first inclusive LLC design proposal to offer such a guarantee, thereby completely isolating the core caches from LLC evictions. We observe that the root cause of inclusion victims is the constraint that an LLC victim must be chosen from the set pointed to by the set indexing function. The ZIV LLC relaxes this constraint only when necessary by efficiently and minimally enabling a global victim selection scheme in the inclusive LLC to avoid generation of inclusion victims. Detailed simulations conducted with a chip-multiprocessor model using multi-programmed and multi-threaded workloads show that the ZIV LLC gracefully supports large mid-level caches (e.g., half the size of the LLC) and delivers performance close to a non-inclusive LLC for different classes of LLC replacement policies. We also show that the ZIV LLC comfortably outperforms the existing related proposals and its performance lead grows with increasing mid-level cache capacity.},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Chaudhuri, Mainak},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-713X},
	keywords = {Degradation, Inclusive cache hierarchy, Lead, Microprocessors, Process control, Proposals, Protocols, Timing, back-invalidation, inclusion victim},
	pages = {71--84},
}

@article{cekleov_virtual-address_1997,
	title = {Virtual-address caches. {Part} 2:  {Multiprocessor} issues},
	volume = {17},
	issn = {1937-4143},
	url = {https://ieeexplore.ieee.org/document/641599},
	doi = {10.1109/40.641599},
	abstract = {In this two-part survey, we discussed the problems and possible solutions caused by virtual address caches in single-processor systems. In this paper, we continue to explore these topics in the context of multiprocessor systems. Processors may access their cache directly using virtual addresses. We discuss the inherent problems and possible solutions in this approach for multiprocessor systems.},
	number = {6},
	journal = {IEEE Micro},
	author = {Cekleov, M. and Dubois, M.},
	month = nov,
	year = {1997},
	note = {Conference Name: IEEE Micro},
	keywords = {Access protocols, Hardware, Kernel, Monitoring, Multiprocessing systems, Processor scheduling, Sun},
	pages = {69--74},
}

@article{cekleov_virtual-address_1997-1,
	title = {Virtual-address caches. {Part} 1: problems and solutions in uniprocessors},
	volume = {17},
	issn = {1937-4143},
	shorttitle = {Virtual-address caches. {Part} 1},
	url = {https://ieeexplore.ieee.org/document/621215},
	doi = {10.1109/40.621215},
	abstract = {This survey exposes the problems related to virtual caches in the context of uniprocessor (Part 1) and multiprocessor (Part 2) systems. We review proposed solutions that have been implemented or proposed in different contexts. The idea is to catalog all solutions, past and present, and to identify technology trends and attractive future approaches. We first overview the relevant properties of virtual memory and of physical caches. To solve the virtual-to-physical address bottle-neck, processors may access caches directly with virtual addresses. This survey introduces the problems and discusses solutions in the context of single-processor systems.},
	number = {5},
	journal = {IEEE Micro},
	author = {Cekleov, M. and Dubois, M.},
	month = sep,
	year = {1997},
	note = {Conference Name: IEEE Micro},
	keywords = {Dictionaries, Indexing, Memory management, Permission, Process design, Protection, Runtime, Space technology, Sun},
	pages = {64--71},
}

@inproceedings{nori_reduct_2021,
	title = {{REDUCT}: {Keep} it {Close}, {Keep} it {Cool}! : {Efficient} {Scaling} of {DNN} {Inference} on {Multi}-core {CPUs} with {Near}-{Cache} {Compute}},
	shorttitle = {{REDUCT}},
	url = {https://ieeexplore.ieee.org/document/9499927},
	doi = {10.1109/ISCA52012.2021.00022},
	abstract = {Deep Neural Networks (DNN) are used in a variety of applications and services. With the evolving nature of DNNs, the race to build optimal hardware (both in datacenter and edge) continues. General purpose multi-core CPUs offer unique attractive advantages for DNN inference at both datacenter [60] and edge [71]. Most of the CPU pipeline design complexity is targeted towards optimizing general-purpose single thread performance, and is overkill for relatively simpler, but still hugely important, data parallel DNN inference workloads. Addressing this disparity efficiently can enable both raw performance scaling and overall performance/Watt improvements for multi-core CPU DNN inference.We present REDUCT, where we build innovative solutions that bypass traditional CPU resources which impact DNN inference power and limit its performance. Fundamentally, REDUCT’s "Keep it close" policy enables consecutive pieces of work to be executed close to each other. REDUCT enables instruction delivery/decode close to execution and instruction execution close to data. Simple ISA extensions encode the fixed-iteration count loop-y workload behavior enabling an effective bypass of many power-hungry front-end stages of the wide Out-of-Order (OoO) CPU pipeline. Per core performance scales efficiently by distributing light-weight tensor compute near all caches in a multi-level cache hierarchy. This maximizes the cumulative utilization of the existing architectural bandwidth resources in the system and minimizes movement of data.Across a number of DNN models, REDUCT achieves a 2.3× increase in convolution performance/Watt with a 2× to 3.94× scaling in raw performance. Similarly, REDUCT achieves a 1.8× increase in inner-product performance/Watt with 2.8× scaling in performance. REDUCT performance/power scaling is achieved with no increase to cache capacity or bandwidth and a mere 2.63\% increase in area. Crucially, REDUCT operates entirely within the CPU programming and memory model, simplifying software development, while achieving performance similar to or better than state-of-the-art Domain Specific Accelerators (DSA) for DNN inference, providing fresh design choices in the AI era.},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Nori, Anant V. and Bera, Rahul and Balachandran, Shankar and Rakshit, Joydeep and Omer, Om J. and Abuhatzera, Avishaii and Kuttanna, Belliappa and Subramoney, Sreenivas},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-713X},
	keywords = {Bandwidth, Computational modeling, Convolution, Deep learning, Out of order, Pipelines, Tensors},
	pages = {167--180},
}

@inproceedings{jiao_power_2010,
	title = {Power and {Performance} {Characterization} of {Computational} {Kernels} on the {GPU}},
	url = {https://ieeexplore.ieee.org/document/5724833},
	doi = {10.1109/GreenCom-CPSCom.2010.143},
	abstract = {Nowadays Graphic Processing Units (GPU) are gaining increasing popularity in high performance computing (HPC). While modern GPUs can offer much more computational power than CPUs, they also consume much more power. Energy efficiency is one of the most important factors that will affect a broader adoption of GPUs in HPC. In this paper, we systematically characterize the power and energy efficiency of GPU computing. Specifically, using three different applications with various degrees of compute and memory intensiveness, we investigate the correlation between power consumption and different computational patterns under various voltage and frequency levels. Our study revealed that energy saving mechanisms on GPUs behave considerably different than CPUs. The characterization results also suggest possible ways to improve the 'greenness' of GPU computing.},
	booktitle = {2010 {IEEE}/{ACM} {Int}'l {Conference} on {Green} {Computing} and {Communications} {Int}'l {Conference} on {Cyber}, {Physical} and {Social} {Computing}},
	author = {Jiao, Y. and Lin, H. and Balaji, P. and Feng, W.},
	month = dec,
	year = {2010},
	keywords = {Clocks, DVFS, Energy-Efficient Computing, GPUs, Graphics processing unit, Kernel, Memory management, Performance evaluation, Power demand},
	pages = {221--228},
}

@inproceedings{sanchez_vicarte_opening_2021,
	title = {Opening {Pandora}’s {Box}: {A} {Systematic} {Study} of {New} {Ways} {Microarchitecture} {Can} {Leak} {Private} {Data}},
	shorttitle = {Opening {Pandora}’s {Box}},
	url = {https://ieeexplore.ieee.org/document/9499823},
	doi = {10.1109/ISCA52012.2021.00035},
	abstract = {Microarchitectural attacks have plunged Computer Architecture into a security crisis. Yet, as the slowing of Moore’s law justifies the use of ever more exotic microarchitecture, it is likely we have only seen the tip of the iceberg.To better anticipate this security crisis, this paper performs a systematic security-centric analysis of the Computer Architecture literature. Our rationale is that when implementing current and future processors, microarchitects will (quite reasonably) look to previously-proposed ideas. Our study uncovers seven classes of microarchitectural optimization with novel security implications, proposes a conceptual framework through which to study them and demonstrates several proofs-of-concept to show their efficacy. The optimizations we study range from those that leak as much privacy as Spectre/Meltdown (but without exploiting speculative execution) to those that otherwise undermine security-critical programs in a variety of ways. Many have storied histories— ranging from industry patents to media/3rd party speculation regarding current implementation status to recent renewed interest in the academic community. This paper’s goal is to perform an early (hopefully not too late) analysis to inform their development moving forward.},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Sanchez Vicarte, Jose Rodrigo and Shome, Pradyumna and Nayak, Nandeeka and Trippel, Caroline and Morrison, Adam and Kohlbrenner, David and Fletcher, Christopher W.},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-713X},
	keywords = {Computer architecture, Microarchitecture, Prefetching, Privacy, Program processors, Programming, Systematics},
	pages = {347--360},
}

@inproceedings{kaseridis_minimalist_2011,
	title = {Minimalist open-page: {A} {DRAM} page-mode scheduling policy for the many-core era},
	shorttitle = {Minimalist open-page},
	url = {https://ieeexplore.ieee.org/document/7851456},
	abstract = {Contemporary DRAM systems have maintained impressive scaling by managing a careful balance between performance, power, and storage density. In achieving these goals, a significant sacrifice has been made in DRAM's operational complexity. To realize good performance, systems must properly manage the significant number of structural and timing restrictions of the DRAM devices. DRAM's use is further complicated in many-core systems where the memory interface is shared among multiple cores/threads competing for memory bandwidth. The use of the “Page-mode” feature of DRAM devices can mitigate many DRAM constraints. Current open-page policies attempt to garner the highest level of page hits. In an effort to achieve this, such greedy schemes map sequential address sequences to a single DRAM resource. This nonuniform resource usage pattern introduces high levels of conflict when multiple workloads in a many-core system map to the same set of resources. In this paper we present a scheme that provides a careful balance between the benefits (increased performance and decreased power), and the detractors (unfairness) of pagemode accesses. In our Minimalist approach, we target “just enough” page-mode accesses to garner page-mode benefits, avoiding system unfairness. We use a fair memory hashing scheme to control the maximum number of page mode hits, and direct the memory scheduler with processor-generated prefetch meta-data.},
	booktitle = {2011 44th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Kaseridis, Dimitris and Stuecheli, Jeffrey and John, Lizy Kurian},
	month = dec,
	year = {2011},
	keywords = {Arrays, Complexity theory, Delays, Design, Performance, Prefetching, Random access memory},
	pages = {24--35},
}

@article{agarwal_leakage_2006,
	title = {Leakage {Power} {Analysis} and {Reduction} for {Nanoscale} {Circuits}},
	volume = {26},
	issn = {1937-4143},
	url = {https://ieeexplore.ieee.org/document/1624329},
	doi = {10.1109/MM.2006.39},
	abstract = {Leakage current in the nanometer regime has become a significant portion of power dissipation in CMOS circuits as threshold voltage, channel length, and gate oxide thickness scale downward. Various techniques are available to reduce leakage power in high-performance systems},
	number = {2},
	journal = {IEEE Micro},
	author = {Agarwal, A. and Mukhopadhyay, S. and Raychowdhury, A. and Roy, K. and Kim, C.H.},
	month = mar,
	year = {2006},
	note = {Conference Name: IEEE Micro},
	keywords = {CMOS, CMOS technology, Circuits, Doping profiles, Energy consumption, Leakage current, Logic design, Maintenance, Nanoscale devices, Subthreshold current, Thickness control, leakage power reduction, nanoscale circuits, technology scaling},
	pages = {68--80},
}

@inproceedings{jimenez_insertion_2013,
	title = {Insertion and promotion for tree-based {PseudoLRU} last-level caches},
	url = {https://ieeexplore.ieee.org/document/7847633},
	abstract = {Last-level caches mitigate the high latency of main memory. A good cache replacement policy enables high performance for memory intensive programs. To be useful to industry, a cache replacement policy must deliver high performance without high complexity or cost. For instance, the costly least-recently-used (LRU) replacement policy is not used in highly associative caches; rather, inexpensive policies with similar performance such as PseudoLRU are used. We propose a novel last-level cache replacement algorithm with approximately the same complexity and storage requirements as tree-based PseudoLRU, but with performance matching state of the art techniques such as dynamic re-reference interval prediction (DRRIP) and protecting distance policy (PDP). The algorithm is based on PseudoLRU, but uses set-dueling to dynamically adapt its insertion and promotion policy. It has slightly less than one bit of overhead per cache block, compared with two or more bits per cache block for competing policies. In this paper, we give the motivation behind the algorithm in the context of LRU with improved placement and promotion, then develop this motivation into a PseudoLRU-based algorithm, and finally give a version using set-dueling to allow adaptivity to changing program behavior. We show that, with a 16-way set-associative 4MB last-level cache, our adaptive PseudoLRU insertion and promotion algorithm yields a geometric mean speedup of 5.6\% over true LRU over all the SPEC CPU 2006 benchmarks using far less overhead than LRU or other algorithms. On a memory-intensive subset of SPEC, the technique gives a geometric mean speedup of 15.6\%. We show that the performance is comparable to state-of-the-art replacement policies that consume more than twice the area of our technique.},
	booktitle = {2013 46th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	author = {Jiménez, Daniel A.},
	month = dec,
	year = {2013},
	keywords = {Benchmark testing, Complexity theory, Context, Genetic algorithms, Heuristic algorithms, Prediction algorithms, Space exploration},
	pages = {284--296},
}

@article{doweck_inside_2017,
	title = {Inside 6th-{Generation} {Intel} {Core}: {New} {Microarchitecture} {Code}-{Named} {Skylake}},
	volume = {37},
	issn = {1937-4143},
	shorttitle = {Inside 6th-{Generation} {Intel} {Core}},
	url = {https://ieeexplore.ieee.org/document/7924286},
	doi = {10.1109/MM.2017.38},
	abstract = {Skylake's core, processor graphics, and system on chip were designed to meet a demanding set of requirements for a wide range of power-performance points. Its coherent fabric was designed to provide high-memory bandwidth from multiple memory sources. Skylake's power management, which includes Intel Speed Shift technology, was designed to provide the largest dynamic power range among prior Intel processors. The Intel Architecture core delivers higher power efficiency, higher frequency, and a wider dynamic power range, supporting smaller form factors. Skylake's Gen9 graphics provides new features designed to maximize energy efficiency and bring the best visual experience for gaming and media. Skylake offers a rich performance monitoring unit that enhances software developers' ability to optimize their applications.},
	number = {2},
	journal = {IEEE Micro},
	author = {Doweck, Jack and Kao, Wen-Fu and Lu, Allen Kuan-yu and Mandelblat, Julius and Rahatekar, Anirudha and Rappoport, Lihu and Rotem, Efraim and Yasin, Ahmad and Yoaz, Adi},
	month = mar,
	year = {2017},
	note = {Conference Name: IEEE Micro},
	keywords = {Bandwidth, Central Processing Unit, Dynamic range, GPU, Graphics, Graphics processing units, Intel Speed Shift, Microarchitecture, Performance evaluation, Ports (Computers), Skylake, Turbo, eDRAM, microarchitecture, performance measurements, performance monitoring, power management},
	pages = {52--62},
}

@inproceedings{ganguly_interplay_2019,
	title = {Interplay between {Hardware} {Prefetcher} and {Page} {Eviction} {Policy} in {CPU}-{GPU} {Unified} {Virtual} {Memory}},
	url = {https://ieeexplore.ieee.org/document/8980317},
	abstract = {Memory capacity in GPGPUs is a major challenge for data-intensive applications with their ever increasing memory requirement. To fit a workload into the limited GPU memory space, a programmer needs to manually divide the workload by tiling the working set and perform user-level data migration. To relieve the programmer from this burden, Unified Virtual Memory (UVM) was developed to support on-demand paging and migration, transparent to the user. It further takes care of the memory over-subscription issue by automatically performing page replacement in an oversubscribed GPU memory situation. However, we found that naïve handling of page faults can cause orders of magnitude slowdown in performance. Moreover, we observed that although prefetching of data from CPU to GPU can hide the page fault latency, the difference among various prefetching mechanisms can lead to drastically different performance results. To this end, we performed extensive experiments on GeForceGTX 1080ti GPUs with PCI-e 3.0 16× to discover that there exists an effective prefetch mechanism to enhance locality in GPU memory. However, as the GPU memory is filled to its capacity, such prefetching mechanism quickly proves to be counterproductive due to locality unaware eviction policy. This necessitates the design of new eviction policies that are aware of the hardware prefetcher semantics. We propose two new programmer-agnostic, locality-aware pre-eviction policies which leverage the mechanics of existing hardware prefetcher and thus incur no additional implementation and performance overhead. We demonstrate that combining the proposed tree-based pre-eviction policy with the hardware prefetcher provides an average of 93\% and 18.5\% performance speed-up compared to LRU based 4KB and 2MB page replacement strategies, respectively. We further examine the memory access pattern of GPU workloads under consideration to analyze the achieved performance speed-up.},
	booktitle = {2019 {ACM}/{IEEE} 46th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Ganguly, Debashis and Zhang, Ziyu and Yang, Jun and Melhem, Rami},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-713X},
	keywords = {GPU, hardware prefetcher, page eviction policy, unified virtual memory},
	pages = {224--235},
}

@inproceedings{vavouliotis_exploiting_2021,
	title = {Exploiting {Page} {Table} {Locality} for {Agile} {TLB} {Prefetching}},
	url = {https://ieeexplore.ieee.org/document/9499825},
	doi = {10.1109/ISCA52012.2021.00016},
	abstract = {Frequent Translation Lookaside Buffer (TLB) misses incur high performance and energy costs due to page walks required for fetching the corresponding address translations. Prefetching page table entries (PTEs) ahead of demand TLB accesses can mitigate the address translation performance bottleneck, but each prefetch requires traversing the page table, triggering additional accesses to the memory hierarchy. Therefore, TLB prefetching is a costly technique that may undermine performance when the prefetches are not accurate.In this paper we exploit the locality in the last level of the page table to reduce the cost and enhance the effectiveness of TLB prefetching by fetching cache-line adjacent PTEs "for free". We propose Sampling-Based Free TLB Prefetching (SBFP), a dynamic scheme that predicts the usefulness of these "free" PTEs and prefetches only the ones most likely to prevent TLB misses. We demonstrate that combining SBFP with novel and state-of-the-art TLB prefetchers significantly improves miss coverage and reduces most memory accesses due to page walks.Moreover, we propose Agile TLB Prefetcher (ATP), a novel composite TLB prefetcher particularly designed to maximize the benefits of SBFP. ATP efficiently combines three low-cost TLB prefetchers and disables TLB prefetching for those execution phases that do not benefit from it. Unlike state-of-the-art TLB prefetchers that correlate patterns with only one feature (e.g., strides, PC, distances), ATP correlates patterns with multiple features and dynamically enables the most appropriate TLB prefetcher per TLB miss.To alleviate the address translation performance bottleneck, we propose a unified solution that combines ATP and SBFP. Across an extensive set of industrial workloads provided by Qualcomm, ATP coupled with SBFP improves geometric speedup by 16.2\%, and eliminates on average 37\% of the memory references due to page walks. Considering the SPEC CPU 2006 and SPEC CPU 2017 benchmark suites, ATP with SBFP increases geometric speedup by 11.1\%, and eliminates page walk memory references by 26\%. Applied to big data workloads (GAP suite, XSBench), ATP with SBFP yields a geometric speedup of 11.8\% while reducing page walk memory references by 5\%. Over the best state-of-the-art TLB prefetcher for each benchmark suite, ATP with SBFP achieves speedups of 8.7\%, 3.4\%, and 4.2\% for the Qualcomm, SPEC, and GAP+XSBench workloads, respectively.},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Vavouliotis, Georgios and Alvarez, Lluc and Karakostas, Vasileios and Nikas, Konstantinos and Koziris, Nectarios and Jiménez, Daniel A. and Casas, Marc},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-713X},
	keywords = {Benchmark testing, Big Data, Memory management, Prefetching, address translation, page table locality, prefetching, translation lookaside buffer, virtual memory},
	pages = {85--98},
}

@inproceedings{choi_fine-grained_2004,
	title = {Fine-grained dynamic voltage and frequency scaling for precise energy and performance trade-off based on the ratio of off-chip access to on-chip computation times},
	volume = {1},
	url = {https://ieeexplore.ieee.org/document/1268819},
	doi = {10.1109/DATE.2004.1268819},
	abstract = {This paper presents an intra-process dynamic voltage and frequency scaling (DVFS) technique targeted toward non real-time applications running on an embedded system platform. The key idea is to make use of runtime information about the external memory access statistics in order to perform CPU voltage and frequency scaling with the goal of minimizing the energy consumption while translucently controlling the performance penalty. The proposed DVFS technique relies on dynamically-constructed regression models that allow the CPU to calculate the expected workload and slack time for the next time slot, and thus, adjust its voltage and frequency in order to save energy while meeting soft timing constraints. This is in turn achieved by estimating and exploiting the ratio of the total off-chip access time to the total on-chip computation time. The proposed technique has been implemented on an XScale-based embedded system platform and actual energy savings have been calculated by current measurements in hardware. For memory-bound programs, a CPU energy saving of more than 70\% with a performance degradation of 12\% was achieved. For CPU-bound programs, 15/spl sim/60\% CPU energy saving was achieved at the cost of 5-20\% performance penalty.},
	booktitle = {Automation and {Test} in {Europe} {Conference} and {Exhibition} {Proceedings} {Design}},
	author = {Choi, Kihwan and Soma, R. and Pedram, M.},
	month = feb,
	year = {2004},
	note = {ISSN: 1530-1591},
	keywords = {Current measurement, Dynamic voltage scaling, Embedded system, Energy consumption, Frequency, Real time systems, Runtime, Statistics, Timing, Voltage control},
	pages = {4--9 Vol.1},
}

@article{bhati_dram_2016,
	title = {{DRAM} {Refresh} {Mechanisms}, {Penalties}, and {Trade}-{Offs}},
	volume = {65},
	issn = {1557-9956},
	url = {https://ieeexplore.ieee.org/document/7070756},
	doi = {10.1109/TC.2015.2417540},
	abstract = {Ever-growing application data footprints demand faster main memory with larger capacity. DRAM has been the technology choice for main memory due to its low latency and high density. However, DRAM cells must be refreshed periodically to preserve their content. Refresh operations negatively affect performance and power. Traditionally, the performance and power overhead of refresh have been insignificant. But as the size and speed of DRAM chips continue to increase, refresh becomes a dominating factor of DRAM performance and power dissipation. In this paper, we conduct a comprehensive study of the issues related to refresh operations in modern DRAMs. Specifically, we describe the difference in refresh operations between modern synchronous DRAM and traditional asynchronous DRAM; the refresh modes and timings; and variations in data retention time. Moreover, we quantify refresh penalties versus device speed, size, and total memory capacity. We also categorize refresh mechanisms based on command granularity, and summarize refresh techniques proposed in research papers. Finally, based on our experiments and observations, we propose guidelines for mitigating DRAM refresh penalties.},
	number = {1},
	journal = {IEEE Transactions on Computers},
	author = {Bhati, Ishwar and Chang, Mu-Tien and Chishti, Zeshan and Lu, Shih-Lien and Jacob, Bruce},
	month = jan,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Computers},
	keywords = {Computer architecture, DRAM Refresh, Multicore processor, Performance evaluation, SDRAM, Temperature sensors, Timing, performance, power},
	pages = {108--121},
}

@inproceedings{bakhshalipour_bingo_2019,
	title = {Bingo {Spatial} {Data} {Prefetcher}},
	url = {https://ieeexplore.ieee.org/document/8675188},
	doi = {10.1109/HPCA.2019.00053},
	abstract = {Applications extensively use data objects with a regular and fixed layout, which leads to the recurrence of access patterns over memory regions. Spatial data prefetching techniques exploit this phenomenon to prefetch future memory references and hide the long latency of DRAM accesses. While state-of-the-art spatial data prefetchers are effective at reducing the number of data misses, we observe that there is still significant room for improvement. To select an access pattern for prefetching, existing spatial prefetchers associate observed access patterns to either a short event with a high probability of recurrence or a long event with a low probability of recurrence. Consequently, the prefetchers either offer low accuracy or lose significant prediction opportunities. We identify that associating the observed spatial patterns to just a single event significantly limits the effectiveness of spatial data prefetchers. In this paper, we make a case for associating the observed spatial patterns to both short and long events to achieve high accuracy while not losing prediction opportunities. We propose Bingo spatial data prefetcher in which short and long events are used to select the best access pattern for prefetching. We propose a storage-efficient design for Bingo in such a way that just one history table is needed to maintain the association between the access patterns and the long and short events. Through a detailed evaluation of a set of big-data applications, we show that Bingo improves system performance by 60\% over a baseline with no data prefetcher and 11\% over the best-performing prior spatial data prefetcher.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Bakhshalipour, Mohammad and Shakerinava, Mehran and Lotfi-Kamran, Pejman and Sarbazi-Azad, Hamid},
	month = feb,
	year = {2019},
	note = {ISSN: 2378-203X},
	keywords = {Big-Data Applications, Data Prefetching, Hardware, History, Memory System, Metadata, Prefetching, Random access memory, Spatial Correlation, Spatial databases, System performance},
	pages = {399--411},
}

@inproceedings{ren_i_2021,
	title = {I {See} {Dead} µops: {Leaking} {Secrets} via {Intel}/{AMD} {Micro}-{Op} {Caches}},
	shorttitle = {I {See} {Dead} µops},
	url = {https://ieeexplore.ieee.org/document/9499837},
	doi = {10.1109/ISCA52012.2021.00036},
	abstract = {Modern Intel, AMD, and ARM processors translate complex instructions into simpler internal micro-ops that are then cached in a dedicated on-chip structure called the micro-op cache. This work presents an in-depth characterization study of the micro-op cache, reverse-engineering many undocumented features, and further describes attacks that exploit the micro-op cache as a timing channel to transmit secret information. In particular, this paper describes three attacks – (1) a same thread cross-domain attack that leaks secrets across the user-kernel boundary, (2) a cross-SMT thread attack that transmits secrets across two SMT threads via the micro-op cache, and (3) transient execution attacks that have the ability to leak an unauthorized secret accessed along a misspeculated path, even before the transient instruction is dispatched to execution, breaking several existing invisible speculation and fencing-based solutions that mitigate Spectre.},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Ren, Xida and Moody, Logan and Taram, Mohammadkazem and Jordan, Matthew and Tullsen, Dean M. and Venkat, Ashish},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-713X},
	keywords = {Computer architecture, Microarchitecture, Program processors, System-on-chip, Timing, Transient analysis},
	pages = {361--374},
}

@unpublished{intel_corporation_intel_2021,
	title = {Intel® 64 and {IA}-32 {Architectures} {Software} {Developer}’s {Manual} {Combined} {Volumes}: 1, {2A}, {2B}, {2C}, {2D}, {3A}, {3B}, {3C}, {3D}, and 4},
	shorttitle = {Intel® {Software} {Developer}’s {Manual}},
	url = {https://software.intel.com/content/www/us/en/develop/download/intel-64-and-ia-32-architectures-sdm-combined-volumes-1-2a-2b-2c-2d-3a-3b-3c-3d-and-4.html},
	abstract = {This document contains the following:

Volume 1: Describes the architecture and programming environment of processors supporting IA-32 and Intel® 64 architectures.

Volume 2: Includes the full instruction set reference, A-Z. Describes the format of the instruction and provides reference pages for instructions.

Volume 3: Includes the full system programming guide, parts 1, 2, 3, and 4. Describes the operating-system support environment of Intel® 64 and IA-32 architectures, including: memory management, protection, task management, interrupt and exception handling, multi-processor support, thermal and power management features, debugging, performance monitoring, system management mode, virtual machine extensions (VMX) instructions, Intel® Virtualization Technology (Intel® VT), and Intel® Software Guard Extensions (Intel® SGX).

Volume 4: Describes the model-specific registers of processors supporting IA-32 and Intel® 64 architectures.},
	language = {English},
	author = {{Intel Corporation}},
	month = jun,
	year = {2021},
}

@inproceedings{sethia_apogee_2013,
	title = {{APOGEE}: {Adaptive} prefetching on {GPUs} for energy efficiency},
	shorttitle = {{APOGEE}},
	url = {https://ieeexplore.ieee.org/document/6618805},
	doi = {10.1109/PACT.2013.6618805},
	abstract = {Modern graphics processing units (GPUs) combine large amounts of parallel hardware with fast context switching among thousands of active threads to achieve high performance. However, such designs do not translate well to mobile environments where power constraints often limit the amount of hardware. In this work, we investigate the use of prefetching as a means to increase the energy efficiency of GPUs. Classically, CPU prefetching results in higher performance but worse energy efficiency due to unnecessary data being brought on chip. Our approach, called APOGEE, uses an adaptive mechanism to dynamically detect and adapt to the memory access patterns found in both graphics and scientific applications that are run on modern GPUs to achieve prefetching efficiencies of over 90\%. Rather than examining threads in isolation, APOGEE uses adjacent threads to more efficiently identify address patterns and dynamically adapt the timeliness of prefetching. The net effect of APOGEE is that fewer thread contexts are necessary to hide memory latency and thus sustain performance. This reduction in thread contexts and related hardware translates to simplification of hardware and leads to a reduction in power. For Graphics and GPGPU applications, APOGEE enables an 8X reduction in multi-threading hardware, while providing a performance benefit of 19\%. This translates to a 52\% increase in performance per watt over systems with high multi-threading and 33\% over existing GPU prefetching techniques.},
	booktitle = {Proceedings of the 22nd {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	author = {Sethia, Ankit and Dasika, Ganesh and Samadi, Mehrzad and Mahlke, Scott},
	month = sep,
	year = {2013},
	note = {ISSN: 1089-795X},
	keywords = {Energy Efficiency, GPU, Graphics processing units, Prefetching, Throughput Processing},
	pages = {73--82},
}

@inproceedings{wu_dynamic_2005,
	title = {A dynamic compilation framework for controlling microprocessor energy and performance},
	url = {https://ieeexplore.ieee.org/document/1540966},
	doi = {10.1109/MICRO.2005.7},
	abstract = {Dynamic voltage and frequency scaling (DVFS) is an effective technique for controlling microprocessor energy and performance. Existing DVFS techniques are primarily based on hardware, OS time-interrupts, or static-compiler techniques. However, substantially greater gains can be realized when control opportunities are also explored in a dynamic compilation environment. There are several advantages to deploying DVFS and managing energy/performance tradeoffs through the use of a dynamic compiler. Most importantly, dynamic compiler driven DVFS is fine-grained, code-aware, and adaptive to the current microarchitecture environment. This paper presents a design framework of the run-time DVFS optimizer in a general dynamic compilation system. A prototype of the DVFS optimizer is implemented and integrated into an industrial-strength dynamic compilation system. The obtained optimization system is deployed in a real hardware platform that directly measures CPU voltage and current for accurate power and energy readings. Experimental results, based on physical measurements for over 40 SPEC or Olden benchmarks, show that significant energy savings are achieved with little performance degradation. SPEC2K FP benchmarks benefit with energy savings of up to 70\% (with 0.5\% performance loss). In addition, SPEC2K INT show up to 44\% energy savings (with 5\% performance loss), SPEC95 FP save up to 64\% (with 4.9\% performance loss), and Olden save up to 61\% (with 4.5\% performance loss). On average, the technique leads to an energy delay product (EDP) improvement that is 3times-5times better than static voltage scaling, and is more than 2times (22\% vs. 9\%) better than the reported DVFS results of prior static compiler work. While the proposed technique is an effective method for microprocessor voltage and frequency control, the design framework and methodology described in this paper have broader potential to address other energy and power issues such as di/dt and thermal control},
	booktitle = {38th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO}'05)},
	author = {Wu, Qiang and Reddi, V.J. and Wu, Youfeng and Lee, Jin and Connors, D. and Brooks, D. and Martonosi, M. and Clark, D.W.},
	month = nov,
	year = {2005},
	note = {ISSN: 2379-3155},
	keywords = {Dynamic compiler, Dynamic voltage scaling, Energy management, Energy measurement, Frequency, Hardware, Microarchitecture, Microprocessors, Performance loss, Voltage control},
	pages = {12 pp.--282},
}

@inproceedings{ros_cost-effective_2021,
	title = {A {Cost}-{Effective} {Entangling} {Prefetcher} for {Instructions}},
	url = {https://ieeexplore.ieee.org/document/9499798},
	doi = {10.1109/ISCA52012.2021.00017},
	abstract = {Prefetching instructions in the instruction cache is a fundamental technique for designing high-performance computers. There are three key properties to consider when designing an efficient and effective prefetcher: timeliness, coverage, and accuracy. Timeliness is essential, as bringing instructions too early increases the risk of the instructions being evicted from the cache before their use and requesting them too late can lead to the instructions arriving after they are demanded. Coverage is important to reduce the number of instruction cache misses and accuracy to ensure that the prefetcher does not pollute the cache or interacts negatively with the other hardware mechanisms.This paper presents the Entangling Prefetcher for Instructions that entangles instructions to maximize timeliness. The prefetcher works by finding which instruction should trigger the prefetch for a subsequent instruction, accounting for the latency of each cache miss. The prefetcher is carefully adjusted to account for both coverage and accuracy. Our evaluation shows that with 40KB of storage, Entangling can increase performance up to 23\%, outperforming state-of-the-art prefetchers.},
	booktitle = {2021 {ACM}/{IEEE} 48th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Ros, Alberto and Jimborean, Alexandra},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-713X},
	keywords = {Computer architecture, Computers, Current measurement, Encoding, Hardware, Instruction prefetching, Organizations, Prefetching, caches, correlation, entangling, latency},
	pages = {99--111},
}

@misc{an_chapter_nodate,
	type = {Personal {Blog}},
	title = {Chapter 2. {Memory} {Addressing} - {Shichao}'s {Notes}},
	url = {https://notes.shichao.io/utlk/ch2/},
	abstract = {This chapter discusses addressing techniques by offering details in 80x86 microprocessors address memory chips and how Linux uses the available addressing circuits.},
	language = {English},
	urldate = {2021-09-25},
	journal = {Chapter 2. Memory Addressing - Shichao's Notes},
	author = {An, Shichao},
}

@misc{utah_arch_research_group_usimm_2012,
	address = {Salt Lake City, UT},
	title = {{USIMM} {Simulator}},
	shorttitle = {{USIMMSoftware}},
	url = {http://www.cs.utah.edu/~rajeev/usimm-v1.3.tar.gz},
	author = {{Utah Arch Research Group}},
	month = apr,
	year = {2012},
}

@techreport{chatterjee_usimm_2012,
	address = {Portland, OR},
	title = {{USIMM}: the {Utah} {SImulated} {Memory} {Module}},
	shorttitle = {{USIMM}},
	url = {https://www.cs.utah.edu/%7Erajeev/pubs/usimm.pdf},
	abstract = {USIMM, the Utah SImulated Memory Module, is a DRAM main memory system simulator that is being released for use in the Memory Scheduling Championship (MSC), organized in conjunction with ISCA-39. MSC is part of the JILP Workshops on Computer Architecture Competitions (JWAC). This report describes the simulation infrastructure and how it will be used within the competition.},
	number = {UUCS-12-002},
	author = {Chatterjee, Niladrish and Balasubramonian, Rajeev and Shevgoor, Manjunath and Pugsley, Seth and Udipi, Aniruddha and Shafiee, Ali and Sudan, Kshitij and Awasthi, Manu and Chishti, Zeshan},
	month = feb,
	year = {2012},
}

@inproceedings{farooq_store-load-branch_2013,
	title = {Store-{Load}-{Branch} ({SLB}) predictor: {A} compiler assisted branch prediction for data dependent branches},
	shorttitle = {Store-{Load}-{Branch} ({SLB}) predictor},
	doi = {10.1109/HPCA.2013.6522307},
	abstract = {Data-dependent branches constitute single biggest source of remaining branch mispredictions. Typically, data-dependent branches are associated with program data structures, and follow store-load-branch execution sequence. A set of memory locations is written at an earlier point in a program. Later, these locations are read, and used for evaluating branch condition. Branch outcome depends on data values stored in data structure, which, typically do not have repeatable pattern. Therefore, in addition to history-based dynamic predictor, we need a different kind of predictor for handling such branches. This paper presents Store-Load-Branch (SLB) predictor; a compiler-assisted dynamic branch prediction scheme for data-dependent direct and indirect branches. For every data-dependent branch, compiler identifies store instructions that modify the data structure associated with the branch. Marked store instructions are dynamically tracked, and stored values are used for computing branch flags ahead of time. Branch flags are buffered, and later used for making predictions. On average, compared to standalone TAGE predictor, combined TAGE+SLB predictor reduces branch MPKI by 21\% and 51\% for SPECINT and EEMBC benchmark suites respectively.},
	booktitle = {2013 {IEEE} 19th {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Farooq, M. Umar and {Khubaib} and John, Lizy K.},
	month = feb,
	year = {2013},
	note = {ISSN: 1530-0897},
	keywords = {Accuracy, Arrays, Benchmark testing, Discrete cosine transforms, Hardware, History},
	pages = {59--70},
}

@inproceedings{lin_branch_2019,
	title = {Branch {Prediction} {Is} {Not} {A} {Solved} {Problem}: {Measurements}, {Opportunities}, and {Future} {Directions}},
	shorttitle = {Branch {Prediction} {Is} {Not} {A} {Solved} {Problem}},
	doi = {10.1109/IISWC47752.2019.9042108},
	abstract = {Modern branch predictors predict the vast majority of conditional branch instructions with near-perfect accuracy, allowing superscalar, out-of-order processors to maximize speculative efficiency and thus performance. However, this impressive overall effectiveness belies a substantial missed opportunity in single-threaded instructions per cycle (IPC). For example, we show that correcting the mispredictions made by the state-of-the-art TAGE-SC-L branch predictor on SPECint 2017 would improve IPC by margins similar to an advance in process technology node. In this work, we measure and characterize these mispredictions. We find that they categorically arise from either (1) a small number of systematically hard-to-predict (H2P) branches; or (2) rare branches with low dynamic execution counts. Using data from SPECint 2017 and additional large code footprint applications, we quantify the occurrence and IPC impact of these two categories. We then demonstrate that solely increasing the resources afforded to existing branch predictors does not address the root causes of most mispredictions. This leads us to reexamine basic assumptions in branch prediction and to propose new research directions that, for example, deploy machine learning to improve pattern matching for H2Ps, and use on-chip phase learning to track long-term statistics for rare branches.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Workload} {Characterization} ({IISWC})},
	author = {Lin, Chit-Kwan and Tarsa, Stephen J.},
	month = nov,
	year = {2019},
	pages = {228--238},
}

@article{mittal_survey_2018,
	title = {A {Survey} of {Techniques} for {Dynamic} {Branch} {Prediction}},
	url = {http://arxiv.org/abs/1804.00261},
	abstract = {Branch predictor (BP) is an essential component in modern processors since high BP accuracy can improve performance and reduce energy by decreasing the number of instructions executed on wrong-path. However, reducing latency and storage overhead of BP while maintaining high accuracy presents significant challenges. In this paper, we present a survey of dynamic branch prediction techniques. We classify the works based on key features to underscore their differences and similarities. We believe this paper will spark further research in this area and will be useful for computer architects, processor designers and researchers.},
	urldate = {2021-09-24},
	journal = {arXiv:1804.00261 [cs]},
	author = {Mittal, Sparsh},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.00261},
	keywords = {Computer Science - Hardware Architecture},
}

@inproceedings{skarlatos_elastic_2020,
	address = {New York, NY, USA},
	series = {{ASPLOS} '20},
	title = {Elastic {Cuckoo} {Page} {Tables}: {Rethinking} {Virtual} {Memory} {Translation} for {Parallelism}},
	isbn = {978-1-4503-7102-5},
	shorttitle = {Elastic {Cuckoo} {Page} {Tables}},
	url = {https://doi.org/10.1145/3373376.3378493},
	doi = {10.1145/3373376.3378493},
	abstract = {The unprecedented growth in the memory needs of emerging memory-intensive workloads has made virtual memory translation a major performance bottleneck. To address this problem, this paper introduces Elastic Cuckoo Page Tables, a novel page table design that transforms the sequential pointer-chasing operation used by conventional multi-level radix page tables into fully-parallel look-ups. The resulting design harvests, for the first time, the benefits of memory level parallelism for address translation. Elastic cuckoo page tables use Elastic Cuckoo Hashing, a novel extension of cuckoo hashing that supports efficient page table resizing. Elastic cuckoo page tables efficiently resolve hash collisions, provide process-private page tables, support multiple page sizes and page sharing among processes, and dynamically adapt page table sizes to meet application requirements. We evaluate elastic cuckoo page tables with full-system simulations of an 8-core processor using a set of graph analytics, bioinformatics, HPC, and system workloads. Elastic cuckoo page tables reduce the address translation overhead by an average of 41\% over conventional radix page tables. The result is a 3-18\% speed-up in application execution.},
	urldate = {2021-09-24},
	booktitle = {Proceedings of the {Twenty}-{Fifth} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Skarlatos, Dimitrios and Kokolis, Apostolos and Xu, Tianyin and Torrellas, Josep},
	month = mar,
	year = {2020},
	keywords = {cuckoo hashing, page tables, virtual memory},
	pages = {1093--1108},
}

@inproceedings{parasar_seesaw_2018,
	address = {Los Angeles, California},
	series = {{ISCA} '18},
	title = {{SEESAW}: using superpages to improve {VIPT} caches},
	isbn = {978-1-5386-5984-7},
	shorttitle = {{SEESAW}},
	url = {https://doi.org/10.1109/ISCA.2018.00026},
	doi = {10.1109/ISCA.2018.00026},
	abstract = {Hardware caches balance fast lookup, high hit rates, energy efficiency, and simplicity of implementation. For L1 caches however, achieving this balance is difficult because of constraints imposed by virtual memory. L1 caches are usually virtually-indexed and physically tagged (VIPT), but this means that they must be highly associative to achieve good capacity. Unfortunately, excessive associativity compromises performance by degrading access times without significantly boosting hit rates, and increases access energy. We propose Seesaw to overcome this problem. Seesaw leverages the increasing ubiquity of superpages1 - since super-pages have more page offset bits, they can accommodate VIPT caches with more sets than what is traditionally possible with only base page sizes. Seesaw dynamically reduces the number of ways that are looked up based on the page size, improving performance and energy. Seesaw requires modest hardware and no OS or application changes.},
	urldate = {2021-09-24},
	booktitle = {Proceedings of the 45th {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {IEEE Press},
	author = {Parasar, Mayank and Bhattacharjee, Abhishek and Krishna, Tushar},
	month = jun,
	year = {2018},
	keywords = {L1 caches, memory systems, superpages, virtual memory},
	pages = {193--206},
}

@inproceedings{chang_improving_2014,
	title = {Improving {DRAM} performance by parallelizing refreshes with accesses},
	doi = {10.1109/HPCA.2014.6835946},
	abstract = {Modern DRAM cells are periodically refreshed to prevent data loss due to leakage. Commodity DDR (double data rate) DRAM refreshes cells at the rank level. This degrades performance significantly because it prevents an entire DRAM rank from serving memory requests while being refreshed. DRAM designed for mobile platforms, LPDDR (low power DDR) DRAM, supports an enhanced mode, called per-bank refresh, that refreshes cells at the bank level. This enables a bank to be accessed while another in the same rank is being refreshed, alleviating part of the negative performance impact of refreshes. Unfortunately, there are two shortcomings of per-bank refresh employed in today's systems. First, we observe that the perbank refresh scheduling scheme does not exploit the full potential of overlapping refreshes with accesses across banks because it restricts the banks to be refreshed in a sequential round-robin order. Second, accesses to a bank that is being refreshed have to wait. To mitigate the negative performance impact of DRAM refresh, we propose two complementary mechanisms, DARP (Dynamic Access Refresh Parallelization) and SARP (Subarray Access Refresh Parallelization). The goal is to address the drawbacks of per-bank refresh by building more efficient techniques to parallelize refreshes and accesses within DRAM. First, instead of issuing per-bank refreshes in a round-robin order, as it is done today, DARP issues per-bank refreshes to idle banks in an out-of-order manner. Furthermore, DARP proactively schedules refreshes during intervals when a batch of writes are draining to DRAM. Second, SARP exploits the existence of mostly-independent subarrays within a bank. With minor modifications to DRAM organization, it allows a bank to serve memory accesses to an idle subarray while another subarray is being refreshed. Extensive evaluations on a wide variety of workloads and systems show that our mechanisms improve system performance (and energy efficiency) compared to three state-of-the-art refresh policies and the performance benefit increases as DRAM density increases.},
	booktitle = {2014 {IEEE} 20th {International} {Symposium} on {High} {Performance} {Computer} {Architecture} ({HPCA})},
	author = {Chang, Kevin Kai-Wei and Lee, Donghyuk and Chishti, Zeshan and Alameldeen, Alaa R. and Wilkerson, Chris and Kim, Yoongu and Mutlu, Onur},
	month = feb,
	year = {2014},
	note = {ISSN: 2378-203X},
	keywords = {DRAM chips, Organizations, Out of order, Schedules, Standards, System performance},
	pages = {356--367},
}

@inproceedings{liu_raidr_2012,
	title = {{RAIDR}: {Retention}-aware intelligent {DRAM} refresh},
	shorttitle = {{RAIDR}},
	doi = {10.1109/ISCA.2012.6237001},
	abstract = {Dynamic random-access memory (DRAM) is the building block of modern main memory systems. DRAM cells must be periodically refreshed to prevent loss of data. These refresh operations waste energy and degrade system performance by interfering with memory accesses. The negative effects of DRAM refresh increase as DRAM device capacity increases. Existing DRAM devices refresh all cells at a rate determined by the leakiest cell in the device. However, most DRAM cells can retain data for significantly longer. Therefore, many of these refreshes are unnecessary. In this paper, we propose RAIDR (Retention-Aware Intelligent DRAM Refresh), a low-cost mechanism that can identify and skip unnecessary refreshes using knowledge of cell retention times. Our key idea is to group DRAM rows into retention time bins and apply a different refresh rate to each bin. As a result, rows containing leaky cells are refreshed as frequently as normal, while most rows are refreshed less frequently. RAIDR uses Bloom filters to efficiently implement retention time bins. RAIDR requires no modification to DRAM and minimal modification to the memory controller. In an 8-core system with 32 GB DRAM, RAIDR achieves a 74.6\% refresh reduction, an average DRAM power reduction of 16.1\%, and an average system performance improvement of 8.6\% over existing systems, at a modest storage overhead of 1.25 KB in the memory controller. RAIDR's benefits are robust to variation in DRAM system configuration, and increase as memory capacity increases.},
	booktitle = {2012 39th {Annual} {International} {Symposium} on {Computer} {Architecture} ({ISCA})},
	author = {Liu, Jamie and Jaiyen, Ben and Veras, Richard and Mutlu, Onur},
	month = jun,
	year = {2012},
	note = {ISSN: 1063-6897},
	keywords = {Arrays, Capacitors, Memory management, Performance evaluation, Radiation detectors, Random access memory, Throughput},
	pages = {1--12},
}

@inproceedings{shi_hierarchical_2021,
	address = {Virtual USA},
	title = {A hierarchical neural model of data prefetching},
	isbn = {978-1-4503-8317-2},
	url = {https://dl.acm.org/doi/10.1145/3445814.3446752},
	doi = {10.1145/3445814.3446752},
	language = {en},
	urldate = {2021-09-24},
	booktitle = {Proceedings of the 26th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Shi, Zhan and Jain, Akanksha and Swersky, Kevin and Hashemi, Milad and Ranganathan, Parthasarathy and Lin, Calvin},
	month = apr,
	year = {2021},
	pages = {861--873},
}

@article{mittal_survey_2016,
	title = {A {Survey} of {Recent} {Prefetching} {Techniques} for {Processor} {Caches}},
	volume = {49},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/2907071},
	doi = {10.1145/2907071},
	abstract = {As the trends of process scaling make memory systems an even more crucial bottleneck, the importance of latency hiding techniques such as prefetching grows further. However, naively using prefetching can harm performance and energy efficiency and, hence, several factors and parameters need to be taken into account to fully realize its potential. In this article, we survey several recent techniques that aim to improve the implementation and effectiveness of prefetching. We characterize the techniques on several parameters to highlight their similarities and differences. The aim of this survey is to provide insights to researchers into working of prefetching techniques and spark interesting future work for improving the performance advantages of prefetching even further.},
	number = {2},
	urldate = {2021-09-24},
	journal = {ACM Computing Surveys},
	author = {Mittal, Sparsh},
	month = aug,
	year = {2016},
	keywords = {Review, cache pollution, classification, data prefetching, hardware (HW) prefetching, helper thread prefetching, instruction prefetching, software (SW) prefetching, speculative pre-execution},
	pages = {35:1--35:35},
}

@inproceedings{al-zoubi_performance_2004,
	address = {New York, NY, USA},
	series = {{ACM}-{SE} 42},
	title = {Performance evaluation of cache replacement policies for the {SPEC} {CPU2000} benchmark suite},
	isbn = {978-1-58113-870-2},
	url = {https://doi.org/10.1145/986537.986601},
	doi = {10.1145/986537.986601},
	abstract = {Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used (LRU), Round-Robin, and PLRU (Pseudo LRU), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set.In our quest for replacement policy as close to optimal as possible, we thoroughly explored the design space of existing replacement mechanisms using SimpleScalar toolset and SPEC CPU2000 benchmark suite, across wide range of cache sizes and organizations. In order to better understand the behavior of different policies, we introduced new measures, such as cumulative distribution of cache hits in the LRU stack. We also dynamically monitored the number of cache misses, per each 100000 instructions.Our results show that the PLRU techniques can approximate and even outperform LRU with much lower complexity, for a wide range of cache organizations. However, a relatively large gap between LRU and optimal replacement policy, of up to 50\%, indicates that new research aimed to close the gap is necessary. The cumulative distribution of cache hits in the LRU stack indicates a very good potential for way prediction using LRU information, since the percentage of hits to the bottom of the LRU stack is relatively high.},
	urldate = {2021-09-24},
	booktitle = {Proceedings of the 42nd annual {Southeast} regional conference},
	publisher = {Association for Computing Machinery},
	author = {Al-Zoubi, Hussein and Milenkovic, Aleksandar and Milenkovic, Milena},
	month = apr,
	year = {2004},
	keywords = {cache memory, performance evaluation, replacement policy},
	pages = {267--272},
}

@inproceedings{sanchez_zcache_2010,
	title = {The {ZCache}: {Decoupling} {Ways} and {Associativity}},
	shorttitle = {The {ZCache}},
	doi = {10.1109/MICRO.2010.20},
	abstract = {The ever-increasing importance of main memory latency and bandwidth is pushing CMPs towards caches with higher capacity and associativity. Associativity is typically improved by increasing the number of ways. This reduces conflict misses, but increases hit latency and energy, placing a stringent trade-off on cache design. We present the zcache, a cache design that allows much higher associativity than the number of physical ways (e.g. a 64-associative cache with 4 ways). The zcache draws on previous research on skew-associative caches and cuckoo hashing. Hits, the common case, require a single lookup, incurring the latency and energy costs of a cache with a very low number of ways. On a miss, additional tag lookups happen off the critical path, yielding an arbitrarily large number of replacement candidates for the incoming block. Unlike conventional designs, the zcache provides associativity by increasing the number of replacement candidates, but not the number of cache ways. To understand the implications of this approach, we develop a general analysis framework that allows to compare associativity across different cache designs (e.g. a set-associative cache and a zcache) by representing associativity as a probability distribution. We use this framework to show that for zcaches, associativity depends only on the number of replacement candidates, and is independent of other factors (such as the number of cache ways or the workload). We also show that, for the same number of replacement candidates, the associativity of a zcache is superior than that of a set-associative cache for most workloads. Finally, we perform detailed simulations of multithreaded and multiprogrammed workloads on a large-scale CMP with zcache as the last-level cache. We show that zcaches provide higher performance and better energy efficiency than conventional caches without incurring the overheads of designs with a large number of ways.},
	booktitle = {2010 43rd {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	author = {Sanchez, Daniel and Kozyrakis, Christos},
	month = dec,
	year = {2010},
	note = {ISSN: 2379-3155},
	keywords = {Arrays, Bandwidth, Delay, Indexes, Process control, Program processors, Radiation detectors, associativity, cache, energy efficiency, multi-core, performance},
	pages = {187--198},
}

@inproceedings{kedzierski_adapting_2010,
	title = {Adapting cache partitioning algorithms to pseudo-{LRU} replacement policies},
	doi = {10.1109/IPDPS.2010.5470352},
	abstract = {Recent studies have shown that cache partitioning is an efficient technique to improve throughput, fairness and Quality of Service (QoS) in CMP processors. The cache partitioning algorithms proposed so far assume Least Recently Used (LRU) as the underlying replacement policy. However, it has been shown that the true LRU imposes extraordinary complexity and area overheads when implemented on high associativity caches, such as last level caches. As a consequence, current processors available on the market use pseudo-LRU replacement policies, which provide similar behavior as LRU, while reducing the hardware complexity. Thus, the presented so far LRU-based cache partitioning solutions cannot be applied to real CMP architectures. This paper proposes a complete partitioning system for caches using the pseudo-LRU replacement policy. In particular, the paper focuses on the pseudo-LRU implementations proposed by Sun Microsystems and IBM, called Not Recently Used (NRU) and Binary Tree (BT), respectively. We propose a high accuracy profiling logic and a cache partitioning hardware for both schemes. We evaluate our proposals' hardware costs in terms of area and power, and compare them against the LRU partitioning algorithm. Overall, this paper presents two hardware techniques to adapt the existing cache partitioning algorithms to real replacement policies. The results show that our solutions impose negligible performance degradation with respect to the LRU.},
	booktitle = {2010 {IEEE} {International} {Symposium} on {Parallel} {Distributed} {Processing} ({IPDPS})},
	author = {Kędzierski, Kamil and Moreto, Miquel and Cazorla, Francisco J. and Valero, Mateo},
	month = apr,
	year = {2010},
	note = {ISSN: 1530-2075},
	keywords = {Binary trees, CMP, Costs, Degradation, Hardware, Logic, Partitioning algorithms, Proposals, Pseudo-LRU, Quality of service, Shared last level cache, Sun, Throughput},
	pages = {1--12},
}

@inproceedings{jaleel_high_2010,
	address = {New York, NY, USA},
	series = {{ISCA} '10},
	title = {High performance cache replacement using re-reference interval prediction ({RRIP})},
	isbn = {978-1-4503-0053-7},
	url = {https://doi.org/10.1145/1815961.1815971},
	doi = {10.1145/1815961.1815971},
	abstract = {Practical cache replacement policies attempt to emulate optimal replacement by predicting the re-reference interval of a cache block. The commonly used LRU replacement policy always predicts a near-immediate re-reference interval on cache hits and misses. Applications that exhibit a distant re-reference interval perform badly under LRU. Such applications usually have a working-set larger than the cache or have frequent bursts of references to non-temporal data (called scans). To improve the performance of such workloads, this paper proposes cache replacement using Re-reference Interval Prediction (RRIP). We propose Static RRIP (SRRIP) that is scan-resistant and Dynamic RRIP (DRRIP) that is both scan-resistant and thrash-resistant. Both RRIP policies require only 2-bits per cache block and easily integrate into existing LRU approximations found in modern processors. Our evaluations using PC games, multimedia, server and SPEC CPU2006 workloads on a single-core processor with a 2MB last-level cache (LLC) show that both SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 4\% and 10\% respectively. Our evaluations with over 1000 multi-programmed workloads on a 4-core CMP with an 8MB shared LLC show that SRRIP and DRRIP outperform LRU replacement on the throughput metric by an average of 7\% and 9\% respectively. We also show that RRIP outperforms LFU, the state-of the art scan-resistant replacement algorithm to-date. For the cache configurations under study, RRIP requires 2X less hardware than LRU and 2.5X less hardware than LFU.},
	urldate = {2021-09-24},
	booktitle = {Proceedings of the 37th annual international symposium on {Computer} architecture},
	publisher = {Association for Computing Machinery},
	author = {Jaleel, Aamer and Theobald, Kevin B. and Steely, Simon C. and Emer, Joel},
	month = jun,
	year = {2010},
	keywords = {replacement, scan resistance, shared cache, thrashing},
	pages = {60--71},
}

@inproceedings{tang_clkscrew_2017,
	address = {Vancouver, BC},
	title = {{CLKSCREW}: {Exposing} the {Perils} of {Security}-{Oblivious} {Energy} {Management}},
	isbn = {978-1-931971-40-9},
	shorttitle = {{CLKscrew}},
	url = {https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/tang},
	abstract = {The need for power- and energy-efficient computing has resulted in aggressive cooperative hardware-software energy management mechanisms on modern commodity devices. Most systems today, for example, allow software to control the frequency and voltage of the underlying hardware at a very fine granularity to extend battery life. Despite their benefits, these software-exposed energy management mechanisms pose grave security implications that have not been studied before.

In this work, we present the CLKSCREW attack, a new class of fault attacks that exploit the security-obliviousness of energy management mechanisms to break security. A novel benefit for the attackers is that these fault attacks become more accessible since they can now be conducted without the need for physical access to the devices or fault injection equipment. We demonstrate CLKSCREW on commodity ARM/Android devices. We show that a malicious kernel driver (1) can extract secret cryptographic keys from Trustzone, and (2) can escalate its privileges by loading self-signed code into Trustzone. As the first work to show the security ramifications of energy management mechanisms, we urge the community to re-examine these security-oblivious designs.},
	language = {English},
	publisher = {USENIX Association},
	author = {Tang, Andrew and Sethumadhavan, Simha and Stolfo, Salvatore},
	month = aug,
	year = {2017},
	pages = {1057--1074},
}

@inproceedings{amit_optimizing_2017,
	address = {USA},
	series = {{USENIX} {ATC} '17},
	title = {Optimizing the {TLB} shootdown algorithm with page access tracking},
	isbn = {978-1-931971-38-6},
	abstract = {The operating system is tasked with maintaining the coherency of per-core TLBs, necessitating costly synchronization operations, notably to invalidate stale mappings. As core-counts increase, the overhead of TLB synchronization likewise increases and hinders scalability, whereas existing software optimizations that attempt to alleviate the problem (like batching) are lacking. We address this problem by revising the TLB synchronization subsystem. We introduce several techniques that detect cases whereby soon-tobe invalidated mappings are cached by only one TLB or not cached at all, allowing us to entirely avoid the cost of synchronization. In contrast to existing optimizations, our approach leverages hardware page access tracking. We implement our techniques in Linux and find that they reduce the number of TLB invalidations by up to 98\% on average and thus improve performance by up to 78\%. Evaluations show that while our techniques may introduce overheads of up to 9\% when memory mappings are never removed, these overheads can be avoided by simple hardware enhancements.},
	urldate = {2021-09-23},
	booktitle = {Proceedings of the 2017 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Amit, Nadav},
	month = jul,
	year = {2017},
	pages = {27--39},
}

@techreport{mcfarling_combining_1993,
	type = {Technical {Note}},
	title = {Combining {Branch} {Predictors}},
	shorttitle = {{CombineBP}},
	url = {https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-TN-36.pdf},
	abstract = {One of the key factors determining computer performance is the degree to which the implementation can take advantage of instruction-level parallelism. Perhaps the most critical limit to this parallelism is the presence of conditional branches that determine which instructions need to be executed next. To increase parallelism, several authors have suggested ways of predicting the direction of conditional branches with hardware that uses the history of previous branches. The different proposed predictors take advantage of different observed patterns in branch behavior. This paper presents a method of combining the advantages of these different types of predictors. The new method uses a history mechanism to keep track of which predictor is most accurate for each branch so that the most accurate predictor can be used. In addition, this paper describes a method of increasing the usefulness of branch history by hashing it together with the branch address. Together, these new techniques are shown to outperform previously known approaches both in terms of maximum prediction accuracy and the prediction accuracy for a given predictor size. Specifically, prediction accuracy reaches 98.1\% correct versus  97.1\% correct for the most accurate previously known approach. Also, this new approach is typically at least a factor of two smaller than other schemes for a given prediction accuracy. Finally, this new approach allows predictors with a single level of history array access to outperform schemes with multiple levels of history for all but the largest predictor sizes.},
	language = {English},
	number = {WRL-TN-36},
	author = {McFarling, Scott},
	year = {1993},
	keywords = {Computer architecture, branch predictor, instruction level parallelism},
	pages = {29},
}

@inproceedings{bhattacharjee_characterizing_2009,
	title = {Characterizing the {TLB} {Behavior} of {Emerging} {Parallel} {Workloads} on {Chip} {Multiprocessors}},
	doi = {10.1109/PACT.2009.26},
	abstract = {Translation Lookaside Buffers (TLBs) are a staple in modern computer systems and have a significant impact on overall system performance. Numerous prior studies have addressed TLB designs to lower access times and miss rates; these, however, have been targeted towards uniprocessor architectures. As the computer industry embraces chip multiprocessor (CMP) architectures, it is important to study the TLB behavior of emerging parallel workloads. This work presents the first full-system characterization of the TLB behavior of emerging parallel applications on real-system CMPs. Using the PARSEC benchmarks, representative of emerging RMS workloads, we show that TLB misses can hinder system performance significantly. We also evaluate TLB miss stream patterns and show that multiple threads of a parallel execution experience a large number of redundant and predictable misses. For our evaluated benchmarks, 30\% to 95\% of the total misses fall under this category. Our results point to the need for novel TLB designs encouraging inter-core cooperation, either through hierarchically shared TLBs or through inter-core TLB prediction mechanisms.},
	booktitle = {2009 18th {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	author = {Bhattacharjee, Abhishek and Martonosi, Margaret},
	month = sep,
	year = {2009},
	note = {ISSN: 1089-795X},
	keywords = {Chip Multiprocessor, Computer architecture, Computer industry, Concurrent computing, Delay, Hardware, Memory management, PARSEC, Parallel architectures, System performance, System-on-a-chip, Translation Lookaside Buffers, Yarn},
	pages = {29--40},
}

@inproceedings{baumann_multikernel_2009,
	address = {New York, NY, USA},
	series = {{SOSP} '09},
	title = {The multikernel: a new {OS} architecture for scalable multicore systems},
	isbn = {978-1-60558-752-3},
	shorttitle = {The multikernel},
	url = {https://doi.org/10.1145/1629575.1629579},
	doi = {10.1145/1629575.1629579},
	abstract = {Commodity computer systems contain more and more processor cores and exhibit increasingly diverse architectural tradeoffs, including memory hierarchies, interconnects, instruction sets and variants, and IO configurations. Previous high-performance computing systems have scaled in specific cases, but the dynamic nature of modern client and server workloads, coupled with the impossibility of statically optimizing an OS for all workloads and hardware variants pose serious challenges for operating system structures. We argue that the challenge of future multicore hardware is best met by embracing the networked nature of the machine, rethinking OS architecture using ideas from distributed systems. We investigate a new OS structure, the multikernel, that treats the machine as a network of independent cores, assumes no inter-core sharing at the lowest level, and moves traditional OS functionality to a distributed system of processes that communicate via message-passing. We have implemented a multikernel OS to show that the approach is promising, and we describe how traditional scalability problems for operating systems (such as memory management) can be effectively recast using messages and can exploit insights from distributed systems and networking. An evaluation of our prototype on multicore systems shows that, even on present-day machines, the performance of a multikernel is comparable with a conventional OS, and can scale better to support future hardware.},
	urldate = {2021-09-23},
	booktitle = {Proceedings of the {ACM} {SIGOPS} 22nd symposium on {Operating} systems principles},
	publisher = {Association for Computing Machinery},
	author = {Baumann, Andrew and Barham, Paul and Dagand, Pierre-Evariste and Harris, Tim and Isaacs, Rebecca and Peter, Simon and Roscoe, Timothy and Schüpbach, Adrian and Singhania, Akhilesh},
	month = oct,
	year = {2009},
	keywords = {message passing, multicore processors, scalability},
	pages = {29--44},
}

@inproceedings{villavieja_didi_2011,
	address = {Galveston, TX, USA},
	title = {{DiDi}: {Mitigating} the {Performance} {Impact} of {TLB} {Shootdowns} {Using} a {Shared} {TLB} {Directory}},
	isbn = {978-1-4577-1794-9 978-0-7695-4566-0},
	shorttitle = {{DiDi}},
	url = {http://ieeexplore.ieee.org/document/6113842/},
	doi = {10.1109/PACT.2011.65},
	abstract = {Translation Look aside Buffers (TLBs) are ubiquitously used in modern architectures to cache virtual-to-physical mappings and, as they are looked up on every memory access, are paramount to performance scalability. The emergence of chip-multiprocessors (CMPs) with per-core TLBs, has brought the problem of TLB coherence to front stage. TLBs are kept coherent at the software-level by the operating system (OS). Whenever the OS modifies page permissions in a page table, it must initiate a coherency transaction among TLBs, a process known as a TLB shoot down. Current CMPs rely on the OS to approximate the set of TLBs caching a mapping and synchronize TLBs using costly Inter-Proceessor Interrupts (IPIs) and software handlers. In this paper, we characterize the impact of TLB shoot downs on multiprocessor performance and scalability, and present the design of a scalable TLB coherency mechanism. First, we show that both TLB shoot down cost and frequency increase with the number of processors and project that software-based TLB shoot downs would thwart the performance of large multiprocessors. We then present a scalable architectural mechanism that couples a shared TLB directory with load/store queue support for lightweight TLB invalidation, and thereby eliminates the need for costly IPIs. Finally, we show that the proposed mechanism reduces the fraction of machine cycles wasted on TLB shoot downs by an order of magnitude.},
	urldate = {2021-09-23},
	booktitle = {2011 {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	publisher = {IEEE},
	author = {Villavieja, Carlos and Karakostas, Vasileios and Vilanova, Lluis and Etsion, Yoav and Ramirez, Alex and Mendelson, Avi and Navarro, Nacho and Cristal, Adrian and Unsal, Osman S.},
	month = oct,
	year = {2011},
	pages = {340--349},
}

@inproceedings{romanescu_unified_2010,
	title = {{UNified} {Instruction}/{Translation}/{Data} ({UNITD}) coherence: {One} protocol to rule them all},
	shorttitle = {{UNified} {Instruction}/{Translation}/{Data} ({UNITD}) coherence},
	doi = {10.1109/HPCA.2010.5416643},
	abstract = {We propose UNITD, a unified hardware coherence framework that integrates translation coherence into the existing cache coherence protocol. In UNITD coherence protocols, the TLBs participate in the cache coherence protocol just like the instruction and data caches, without requiring any changes to the existing coherence protocol. UNITD eliminates the need for the software TLB shootdown routine, a procedure known to be performance costly and non-scalable. We evaluate snooping and directory UNITD coherence protocols on multicore processors with 2-16 cores, and we demonstrate that UNITD reduces the performance penalty associated with TLB coherence to almost zero.},
	booktitle = {{HPCA} - 16 2010 {The} {Sixteenth} {International} {Symposium} on {High}-{Performance} {Computer} {Architecture}},
	author = {Romanescu, Bogdan F. and Lebeck, Alvin R. and Sorin, Daniel J. and Bracy, Anne},
	month = jan,
	year = {2010},
	note = {ISSN: 2378-203X},
	keywords = {Computer architecture, Hardware, Local activities, Memory management, Microarchitecture, Multicore processing, Multiprocessing systems, Protocols, Software maintenance, Software performance},
	pages = {1--12},
}

@inproceedings{gugale_attc_2020,
	address = {New York, NY, USA},
	series = {{PACT} '20},
	title = {{ATTC} (@{C}): {Addressable}-{TLB} based {Translation} {Coherence}},
	isbn = {978-1-4503-8075-1},
	shorttitle = {{ATTC} (@{C})},
	url = {https://doi.org/10.1145/3410463.3414653},
	doi = {10.1145/3410463.3414653},
	abstract = {Heterogeneous memory systems are getting popular, however they face significant challenges from translation coherence overheads from page remappings. Translation coherence, which is typically implemented in software, can consume up to 50\% of the runtime for some applications in virtualized platforms. In this paper, we propose ATTC -- Addressable TLB-based Translation Coherence, a hardware translation coherence scheme which eliminates almost all of the overheads associated with software-based coherence mechanisms, and overcomes the challenges in existing hardware schemes. Unlike other proposals (HATRIC, UNITD) that require on-chip TLB tags to enforce coherence and are capable of tracking only the last level page table entries of either the guest or host page tables, ATTC tracks changes to both guest and host page tables without requiring any additional metadata in L1, L2 TLBs. ATTC enforces a "point of coherence'' uniformly for both guest and host page table updates using an addressable TLB (ATLB) in the DRAM akin to the one in [41]. An inverse mapping table (INVTBL - present in DRAM) that maps host physical pages to ATLB locations helps to precisely track translations. We study the proposed ATTC scheme in detail for an emerging hybrid memory organization (a mix of DRAM and NVM) and show that ATTC practically eliminates all translation coherence overheads, yielding an average improvement of 35.7\% over a baseline software coherence scheme in virtualized environment and 7.4\% over the hardware HATRIC scheme.},
	urldate = {2021-09-23},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques}},
	publisher = {Association for Computing Machinery},
	author = {Gugale, Harsh and Gulur, Nagendra and Marathe, Yashwant and John, Lizy K.},
	month = sep,
	year = {2020},
	keywords = {hybrid memory, tlb shootdown, translation coherence, virtualization},
	pages = {481--492},
}

@article{yan_hardware_2018,
	title = {Hardware {Translation} {Coherence} for {Virtualized} {Systems}},
	volume = {52},
	issn = {0163-5980},
	url = {https://doi.org/10.1145/3273982.3273988},
	doi = {10.1145/3273982.3273988},
	abstract = {To improve system performance, operating systems (OSes) often undertake activities that require modification of virtual-to-physical address translations. For example, the OS may migrate data between physical pages to manage heterogeneous memory devices. We refer to such activities as page remappings. Unfortunately, page remappings are expensive. We show that a big part of this cost arises from address translation coherence, particularly on systems employing virtualization. In response, we propose hardware translation invalidation and coherence or HATRIC, a readily implementable hardware mechanism to piggyback translation coherence atop existing cache coherence protocols. We perform detailed studies using KVM-based virtualization, showing that HATRIC achieves up to 30\% performance and 10\% energy benefits, for per-CPU area overheads of 0.2\%. We also quantify HATRIC's benefits on systems running Xen and find up to 33\% performance improvements.},
	number = {1},
	urldate = {2021-09-23},
	journal = {ACM SIGOPS Operating Systems Review},
	author = {Yan, Zi and Veselý, Ján and Cox, Guilherme and Bhattacharjee, Abhishek},
	month = aug,
	year = {2018},
	keywords = {Virtualization, heterogeneous memory, translation coherence},
	pages = {57--70},
}

@inproceedings{kumar_latr_2018,
	address = {New York, NY, USA},
	series = {{ASPLOS} '18},
	title = {{LATR}: {Lazy} {Translation} {Coherence}},
	isbn = {9781450349116},
	shorttitle = {{LATR}},
	url = {https://doi.org/10.1145/3173162.3173198},
	doi = {10.1145/3173162.3173198},
	abstract = {We propose LATR-lazy TLB coherence-a software-based TLB shootdown mechanism that can alleviate the overhead of the synchronous TLB shootdown mechanism in existing operating systems. By handling the TLB coherence in a lazy fashion, LATR can avoid expensive IPIs which are required for delivering a shootdown signal to remote cores, and the performance overhead of associated interrupt handlers. Therefore, virtual memory operations, such as free and page migration operations, can benefit significantly from LATR's mechanism. For example, LATR improves the latency of munmap() by 70.8\% on a 2-socket machine, a widely used configuration in modern data centers. Real-world, performance-critical applications such as web servers can also benefit from LATR: without any application-level changes, LATR improves Apache by 59.9\% compared to Linux, and by 37.9\% compared to ABIS, a highly optimized, state-of-the-art TLB coherence technique.},
	urldate = {2021-09-23},
	booktitle = {Proceedings of the {Twenty}-{Third} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Kumar, Mohan Kumar and Maass, Steffen and Kashyap, Sanidhya and Veselý, Ján and Yan, Zi and Kim, Taesoo and Bhattacharjee, Abhishek and Krishna, Tushar},
	month = mar,
	year = {2018},
	keywords = {TLB, asynchrony, translation coherence},
	pages = {651--664},
}

@inproceedings{awad_avoiding_2017,
	address = {Portland, OR},
	title = {Avoiding {TLB} {Shootdowns} {Through} {Self}-{Invalidating} {TLB} {Entries}},
	isbn = {978-1-5090-6764-0},
	url = {http://ieeexplore.ieee.org/document/8091251/},
	doi = {10.1109/PACT.2017.38},
	abstract = {Updates to a process's page table entry (PTE) renders any existing copies of that PTE in any of a system's TLBs stale. To prevent a process from making illegal memory accesses using stale TLB entries, the operating system (OS) performs a costly TLB shootdown operation. Rather than explicitly issuing shootdowns, we propose a coordinated TLB and page table management mechanism where an expirationtime is associated with each TLB entry. An expired TLB entry is treated as invalid. For each PTE, the OS then tracks the latest expiration time of any TLB entry potentially caching that PTE. No shootdown is issued if the OS modifies a PTE when its corresponding latest expiration time has already passed.In this paper, we explain the hardware and OS support required to support Self-invalidating TLB entries (SITE). As an emerging use case that needs fast TLB shootdowns, we consider memory systems consisting of different types of memory (e.g., faster DRAM and slower non-volatile memory) where aggressive migrations are desirable to keep frequently accessed pages in faster memory, but pages cannot migratetoo often because each migration requires a PTE update and corresponding TLB shootdown. We demonstrate that such heterogeneous memory systems augmented with SITE can allow an average performance improvement of 45.5\% over a similar system with traditional TLB shootdowns by avoiding more than 65\% of the shootdowns.},
	urldate = {2021-09-23},
	booktitle = {2017 26th {International} {Conference} on {Parallel} {Architectures} and {Compilation} {Techniques} ({PACT})},
	publisher = {IEEE},
	author = {Awad, Amro and Basu, Arkaprava and Blagodurov, Sergey and Solihin, Yan and Loh, Gabriel H.},
	month = sep,
	year = {2017},
	pages = {273--287},
}

@inproceedings{mazumdar_dead_2021,
	address = {Seoul, Korea (South)},
	title = {Dead {Page} and {Dead} {Block} {Predictors}: {Cleaning} {TLBs} and {Caches} {Together}},
	isbn = {978-1-66542-235-2},
	shorttitle = {Dead {Page} and {Dead} {Block} {Predictors}},
	url = {https://ieeexplore.ieee.org/document/9407144/},
	doi = {10.1109/HPCA51647.2021.00050},
	abstract = {The last level TLB (LLT) and the last level cache (LLC) play a critical role in the overall performance of memory-intensive applications. While management of LLC content has received significant attention, the same may not be true for LLT. In this work, we first explore the well-known concept of dead blocks in caches for TLBs. We find that dead pages are fairly common in the LLT. Different from dead blocks in LLCs, dead pages in LLTs are most often dead-on-arrival, i.e., they produce zero hits in the TLB. We design a storage-efficient dead page predictor that works with a fraction of storage compared to typical dead block predictors. This is important since an LLT itself requires only a few KBs of storage compared to MBs in LLC. We then leverage the dead page information to guide a simple dead block predictor in LLC. This is driven by the observation that dead blocks are often concentrated within dead pages. In effect, we designed a dead page predictor and a correlating dead block predictor with a total storage overhead of only 11KB to bypass predicted dead pages and dead blocks in LLTs and LLCs, respectively. Together, these predictors help improve the IPC of a set of 14 memory-intensive workloads by 8.3\%, on average.},
	urldate = {2021-09-23},
	booktitle = {2021 {IEEE} {International} {Symposium} on {High}-{Performance} {Computer} {Architecture} ({HPCA})},
	publisher = {IEEE},
	author = {Mazumdar, Chandrashis and Mitra, Prachatos and Basu, Arkaprava},
	month = feb,
	year = {2021},
	pages = {507--519},
}

@inproceedings{amit_optimizing_2017-1,
	address = {USA},
	series = {{USENIX} {ATC} '17},
	title = {Optimizing the {TLB} shootdown algorithm with page access tracking},
	isbn = {978-1-931971-38-6},
	abstract = {The operating system is tasked with maintaining the coherency of per-core TLBs, necessitating costly synchronization operations, notably to invalidate stale mappings. As core-counts increase, the overhead of TLB synchronization likewise increases and hinders scalability, whereas existing software optimizations that attempt to alleviate the problem (like batching) are lacking. We address this problem by revising the TLB synchronization subsystem. We introduce several techniques that detect cases whereby soon-tobe invalidated mappings are cached by only one TLB or not cached at all, allowing us to entirely avoid the cost of synchronization. In contrast to existing optimizations, our approach leverages hardware page access tracking. We implement our techniques in Linux and find that they reduce the number of TLB invalidations by up to 98\% on average and thus improve performance by up to 78\%. Evaluations show that while our techniques may introduce overheads of up to 9\% when memory mappings are never removed, these overheads can be avoided by simple hardware enhancements.},
	urldate = {2021-09-23},
	booktitle = {Proceedings of the 2017 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Amit, Nadav},
	month = jul,
	year = {2017},
	pages = {27--39},
}

@article{lustig_tlb_2013,
	title = {{TLB} {Improvements} for {Chip} {Multiprocessors}: {Inter}-{Core} {Cooperative} {Prefetchers} and {Shared} {Last}-{Level} {TLBs}},
	volume = {10},
	issn = {1544-3566},
	shorttitle = {{TLB} {Improvements} for {Chip} {Multiprocessors}},
	url = {https://doi.org/10.1145/2445572.2445574},
	doi = {10.1145/2445572.2445574},
	abstract = {Translation Lookaside Buffers (TLBs) are critical to overall system performance. Much past research has addressed uniprocessor TLBs, lowering access times and miss rates. However, as Chip MultiProcessors (CMPs) become ubiquitous, TLB design and performance must be reevaluated. Our article begins by performing a thorough TLB performance evaluation of sequential and parallel benchmarks running on a real-world, modern CMP system using hardware performance counters. This analysis demonstrates the need for further improvement of TLB hit rates for both classes of application, and it also points out that the data TLB has a significantly higher miss rate than the instruction TLB in both cases. In response to the characterization data, we propose and evaluate both Inter-Core Cooperative (ICC) TLB prefetchers and Shared Last-Level (SLL) TLBs as alternatives to the commercial norm of private, per-core L2 TLBs. ICC prefetchers eliminate 19\% to 90\% of Data TLB (D-TLB) misses across parallel workloads while requiring only modest changes in hardware. SLL TLBs eliminate 7\% to 79\% of D-TLB misses for parallel workloads and 35\% to 95\% of D-TLB misses for multiprogrammed sequential workloads. This corresponds to 27\% and 21\% increases in hit rates as compared to private, per-core L2 TLBs, respectively, and is achieved this using even more modest hardware requirements. Because of their benefits for parallel applications, their applicability to sequential workloads, and their readily implementable hardware, SLL TLBs and ICC TLB prefetchers hold great promise for CMPs.},
	number = {1},
	urldate = {2021-09-23},
	journal = {ACM Transactions on Architecture and Code Optimization},
	author = {Lustig, Daniel and Bhattacharjee, Abhishek and Martonosi, Margaret},
	month = apr,
	year = {2013},
	keywords = {TLB prefetching, Translation lookaside buffer, performance evaluation, shared last-level TLB, simulation},
	pages = {2:1--2:38},
}

@inproceedings{amit_dont_2020,
	address = {New York, NY, USA},
	series = {{EuroSys} '20},
	title = {Don't shoot down {TLB} shootdowns!},
	isbn = {978-1-4503-6882-7},
	url = {https://doi.org/10.1145/3342195.3387518},
	doi = {10.1145/3342195.3387518},
	abstract = {Translation Lookaside Buffers (TLBs) are critical for building performant virtual memory systems. Because most processors do not provide coherence for TLB mappings, TLB shootdowns provide a software mechanism that invokes inter-processor interrupts (IPLs) to synchronize TLBs. TLB shootdowns are expensive, so recent work has aimed to avoid the frequency of shootdowns through techniques such as batching. We show that aggressive batching can cause correctness issues and addressing them can obviate the benefits of batching. Instead, our work takes a different approach which focuses on both improving the performance of TLB shootdowns and carefully selecting where to avoid shootdowns. We introduce four general techniques to improve shootdown performance: (1) concurrently flush initiator and remote TLBs, (2) early acknowledgement from remote cores, (3) cacheline consolidation of kernel data structures to reduce cacheline contention, and (4) in-context flushing of userspace entries to address the overheads introduced by Spectre and Meltdown mitigations. We also identify that TLB flushing can be avoiding when handling copy-on-write (CoW) faults and some TLB shootdowns can be batched in certain system calls. Overall, we show that our approach results in significant speedups without sacrificing safety and correctness in both microbenchmarks and real-world applications.},
	urldate = {2021-09-23},
	booktitle = {Proceedings of the {Fifteenth} {European} {Conference} on {Computer} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Amit, Nadav and Tai, Amy and Wei, Michael},
	month = apr,
	year = {2020},
	pages = {1--14},
}
