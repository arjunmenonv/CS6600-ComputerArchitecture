
@article{kandiraju_characterizing_2002,
	title = {Characterizing the d-{TLB} behavior of {SPEC} {CPU}2000 benchmarks},
	volume = {30},
	issn = {0163-5999},
	url = {https://doi.org/10.1145/511399.511351},
	doi = {10.1145/511399.511351},
	abstract = {Despite the numerous optimization and evaluation studies that have been conducted with {TLBs} over the years, there is still a deficiency in an indepth understanding of {TLB} characteristics from an application angle. This paper presents a detailed characterization study of the {TLB} behavior of the {SPEC} {CPU}2000 benchmark suite. The contributions of this work are in identifying important application characteristics for {TLB} studies, quantifying the {SPEC}2000 application behavior for these characteristics, as well as making pronouncements and suggestions for future research based on these results.Around one-fourth of the {SPEC}2000 applications (ammp, apsi, galgel, lucas, mcf, twolf and vpr) have significant {TLB} missrates. Both capacity and associativity are influencing factors on miss-rates, though they do not necessarily go hand-in-hand. Multi-level {TLBs} are definitely useful for these applications in cutting down access times without significant miss rate degradation. Superpaging to combine {TLB} entries may not be rewarding for many of these applications. Software management of {TLBs} in terms of determining what entries to prefetch, what entries to replace, and what entries to pin has a lot of potential to cut down miss rates considerably. Specifically, the potential benefits of prefetching {TLB} entries is examined, and Distance Prefetching is shown to give good prediction accuracy for these applications.},
	pages = {129--139},
	number = {1},
	journaltitle = {{ACM} {SIGMETRICS} Performance Evaluation Review},
	shortjournal = {{SIGMETRICS} Perform. Eval. Rev.},
	author = {Kandiraju, Gokul B. and Sivasubramaniam, Anand},
	urldate = {2021-09-29},
	date = {2002-06-01},
}

@article{stenstrom_survey_1990,
	title = {A survey of cache coherence schemes for multiprocessors},
	volume = {23},
	issn = {1558-0814},
	doi = {10.1109/2.55497},
	abstract = {Schemes for cache coherence that exhibit various degrees of hardware complexity, ranging from protocols that maintain coherence in hardware, to software policies that prevent the existence of copies of shared, writable data, are surveyed. Some examples of the use of shared data are examined. These examples help point out a number of performance issues. Hardware protocols are considered. It is seen that consistency can be maintained efficiently, although in some cases with considerable hardware complexity, especially for multiprocessors with many processors. Software schemes are investigated as an alternative capable of reducing the hardware cost.{\textless}{\textgreater}},
	pages = {12--24},
	number = {6},
	journaltitle = {Computer},
	author = {Stenstrom, P.},
	date = {1990-06},
	note = {Conference Name: Computer},
	keywords = {Costs, Hardware, Protocols, Software maintenance},
}

@book{jacob_memory_2008,
	location = {Burlington, {MA}},
	title = {Memory systems: Cache, {DRAM}, Disk},
	isbn = {978-0-12-379751-3},
	shorttitle = {Memory systems},
	pagetotal = {982},
	publisher = {Morgan Kaufmann Publishers},
	author = {Jacob, Bruce and Ng, Spencer W. and Wang, David T.},
	date = {2008},
	note = {{OCLC}: ocn154760293},
	keywords = {Computer storage devices},
}

@inproceedings{chaudhuri_zero_2021,
	title = {Zero Inclusion Victim: Isolating Core Caches from Inclusive Last-level Cache Evictions},
	url = {https://ieeexplore.ieee.org/document/9499802},
	doi = {10.1109/ISCA52012.2021.00015},
	shorttitle = {Zero Inclusion Victim},
	abstract = {The most widely used last-level cache ({LLC}) architecture in the microprocessors has been the inclusive {LLC} design. The popularity of the inclusive design stems from the bandwidth optimization and simplification it offers to the implementation of the cache coherence protocols. However, inclusive {LLCs} have always been associated with the curse of inclusion victims. An inclusion victim is a block that must be forcefully replaced from the inner levels of the cache hierarchy when the copy of the block is replaced from the inclusive {LLC}. This tight coupling between the {LLC} victims and the inner-level cache contents leads to three major drawbacks. First, live inclusion victims can lead to severe performance degradation depending on the {LLC} replacement policies. Second, a process can victimize the blocks of another process in an {LLC} shared by multiple cores and this can be exploited to leak information through well-known eviction-based timing side-channels. An inclusive {LLC} makes these channels much less noisy due to the presence of inclusion victims which allow the malicious processes to control the contents of the percore private caches through {LLC} evictions. Third, to reduce the impact of the aforementioned two drawbacks, the inner-level caches, particularly the mid-level cache in a three-level inclusive cache hierarchy, must be kept small even if a larger mid-level cache could have been beneficial in the absence of inclusion victims.We observe that inclusion victims are not fundamental to the inclusion property, but arise due to the way the contents of an inclusive {LLC} are managed. Motivated by this observation, we introduce a fundamentally new inclusive {LLC} design named the Zero Inclusion Victim ({ZIV}) {LLC} that guarantees freedom from inclusion victims while retaining all advantages of an inclusive {LLC}. This is the first inclusive {LLC} design proposal to offer such a guarantee, thereby completely isolating the core caches from {LLC} evictions. We observe that the root cause of inclusion victims is the constraint that an {LLC} victim must be chosen from the set pointed to by the set indexing function. The {ZIV} {LLC} relaxes this constraint only when necessary by efficiently and minimally enabling a global victim selection scheme in the inclusive {LLC} to avoid generation of inclusion victims. Detailed simulations conducted with a chip-multiprocessor model using multi-programmed and multi-threaded workloads show that the {ZIV} {LLC} gracefully supports large mid-level caches (e.g., half the size of the {LLC}) and delivers performance close to a non-inclusive {LLC} for different classes of {LLC} replacement policies. We also show that the {ZIV} {LLC} comfortably outperforms the existing related proposals and its performance lead grows with increasing mid-level cache capacity.},
	eventtitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	pages = {71--84},
	booktitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	author = {Chaudhuri, Mainak},
	date = {2021-06},
	note = {{ISSN}: 2575-713X},
	keywords = {Degradation, Inclusive cache hierarchy, Lead, Microprocessors, Process control, Proposals, Protocols, Timing, back-invalidation, inclusion victim},
}

@article{cekleov_virtual-address_1997,
	title = {Virtual-address caches. Part 2:  Multiprocessor issues},
	volume = {17},
	issn = {1937-4143},
	url = {https://ieeexplore.ieee.org/document/641599},
	doi = {10.1109/40.641599},
	abstract = {In this two-part survey, we discussed the problems and possible solutions caused by virtual address caches in single-processor systems. In this paper, we continue to explore these topics in the context of multiprocessor systems. Processors may access their cache directly using virtual addresses. We discuss the inherent problems and possible solutions in this approach for multiprocessor systems.},
	pages = {69--74},
	number = {6},
	journaltitle = {{IEEE} Micro},
	author = {Cekleov, M. and Dubois, M.},
	date = {1997-11},
	note = {Conference Name: {IEEE} Micro},
	keywords = {Access protocols, Hardware, Kernel, Monitoring, Multiprocessing systems, Processor scheduling, Sun},
}

@article{cekleov_virtual-address_1997-1,
	title = {Virtual-address caches. Part 1: problems and solutions in uniprocessors},
	volume = {17},
	issn = {1937-4143},
	url = {https://ieeexplore.ieee.org/document/621215},
	doi = {10.1109/40.621215},
	shorttitle = {Virtual-address caches. Part 1},
	abstract = {This survey exposes the problems related to virtual caches in the context of uniprocessor (Part 1) and multiprocessor (Part 2) systems. We review proposed solutions that have been implemented or proposed in different contexts. The idea is to catalog all solutions, past and present, and to identify technology trends and attractive future approaches. We first overview the relevant properties of virtual memory and of physical caches. To solve the virtual-to-physical address bottle-neck, processors may access caches directly with virtual addresses. This survey introduces the problems and discusses solutions in the context of single-processor systems.},
	pages = {64--71},
	number = {5},
	journaltitle = {{IEEE} Micro},
	author = {Cekleov, M. and Dubois, M.},
	date = {1997-09},
	note = {Conference Name: {IEEE} Micro},
	keywords = {Dictionaries, Indexing, Memory management, Permission, Process design, Protection, Runtime, Space technology, Sun},
}

@inproceedings{nori_reduct_2021,
	title = {{REDUCT}: Keep it Close, Keep it Cool! : Efficient Scaling of {DNN} Inference on Multi-core {CPUs} with Near-Cache Compute},
	url = {https://ieeexplore.ieee.org/document/9499927},
	doi = {10.1109/ISCA52012.2021.00022},
	shorttitle = {{REDUCT}},
	abstract = {Deep Neural Networks ({DNN}) are used in a variety of applications and services. With the evolving nature of {DNNs}, the race to build optimal hardware (both in datacenter and edge) continues. General purpose multi-core {CPUs} offer unique attractive advantages for {DNN} inference at both datacenter [60] and edge [71]. Most of the {CPU} pipeline design complexity is targeted towards optimizing general-purpose single thread performance, and is overkill for relatively simpler, but still hugely important, data parallel {DNN} inference workloads. Addressing this disparity efficiently can enable both raw performance scaling and overall performance/Watt improvements for multi-core {CPU} {DNN} inference.We present {REDUCT}, where we build innovative solutions that bypass traditional {CPU} resources which impact {DNN} inference power and limit its performance. Fundamentally, {REDUCT}’s "Keep it close" policy enables consecutive pieces of work to be executed close to each other. {REDUCT} enables instruction delivery/decode close to execution and instruction execution close to data. Simple {ISA} extensions encode the fixed-iteration count loop-y workload behavior enabling an effective bypass of many power-hungry front-end stages of the wide Out-of-Order ({OoO}) {CPU} pipeline. Per core performance scales efficiently by distributing light-weight tensor compute near all caches in a multi-level cache hierarchy. This maximizes the cumulative utilization of the existing architectural bandwidth resources in the system and minimizes movement of data.Across a number of {DNN} models, {REDUCT} achieves a 2.3× increase in convolution performance/Watt with a 2× to 3.94× scaling in raw performance. Similarly, {REDUCT} achieves a 1.8× increase in inner-product performance/Watt with 2.8× scaling in performance. {REDUCT} performance/power scaling is achieved with no increase to cache capacity or bandwidth and a mere 2.63\% increase in area. Crucially, {REDUCT} operates entirely within the {CPU} programming and memory model, simplifying software development, while achieving performance similar to or better than state-of-the-art Domain Specific Accelerators ({DSA}) for {DNN} inference, providing fresh design choices in the {AI} era.},
	eventtitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	pages = {167--180},
	booktitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	author = {Nori, Anant V. and Bera, Rahul and Balachandran, Shankar and Rakshit, Joydeep and Omer, Om J. and Abuhatzera, Avishaii and Kuttanna, Belliappa and Subramoney, Sreenivas},
	date = {2021-06},
	note = {{ISSN}: 2575-713X},
	keywords = {Bandwidth, Computational modeling, Convolution, Deep learning, Out of order, Pipelines, Tensors},
}

@inproceedings{jiao_power_2010,
	title = {Power and Performance Characterization of Computational Kernels on the {GPU}},
	url = {https://ieeexplore.ieee.org/document/5724833},
	doi = {10.1109/GreenCom-CPSCom.2010.143},
	abstract = {Nowadays Graphic Processing Units ({GPU}) are gaining increasing popularity in high performance computing ({HPC}). While modern {GPUs} can offer much more computational power than {CPUs}, they also consume much more power. Energy efficiency is one of the most important factors that will affect a broader adoption of {GPUs} in {HPC}. In this paper, we systematically characterize the power and energy efficiency of {GPU} computing. Specifically, using three different applications with various degrees of compute and memory intensiveness, we investigate the correlation between power consumption and different computational patterns under various voltage and frequency levels. Our study revealed that energy saving mechanisms on {GPUs} behave considerably different than {CPUs}. The characterization results also suggest possible ways to improve the 'greenness' of {GPU} computing.},
	eventtitle = {2010 {IEEE}/{ACM} Int'l Conference on Green Computing and Communications Int'l Conference on Cyber, Physical and Social Computing},
	pages = {221--228},
	booktitle = {2010 {IEEE}/{ACM} Int'l Conference on Green Computing and Communications Int'l Conference on Cyber, Physical and Social Computing},
	author = {Jiao, Y. and Lin, H. and Balaji, P. and Feng, W.},
	date = {2010-12},
	keywords = {Clocks, {DVFS}, Energy-Efficient Computing, {GPUs}, Graphics processing unit, Kernel, Memory management, Performance evaluation, Power demand},
}

@inproceedings{sanchez_vicarte_opening_2021,
	title = {Opening Pandora’s Box: A Systematic Study of New Ways Microarchitecture Can Leak Private Data},
	url = {https://ieeexplore.ieee.org/document/9499823},
	doi = {10.1109/ISCA52012.2021.00035},
	shorttitle = {Opening Pandora’s Box},
	abstract = {Microarchitectural attacks have plunged Computer Architecture into a security crisis. Yet, as the slowing of Moore’s law justifies the use of ever more exotic microarchitecture, it is likely we have only seen the tip of the iceberg.To better anticipate this security crisis, this paper performs a systematic security-centric analysis of the Computer Architecture literature. Our rationale is that when implementing current and future processors, microarchitects will (quite reasonably) look to previously-proposed ideas. Our study uncovers seven classes of microarchitectural optimization with novel security implications, proposes a conceptual framework through which to study them and demonstrates several proofs-of-concept to show their efficacy. The optimizations we study range from those that leak as much privacy as Spectre/Meltdown (but without exploiting speculative execution) to those that otherwise undermine security-critical programs in a variety of ways. Many have storied histories— ranging from industry patents to media/3rd party speculation regarding current implementation status to recent renewed interest in the academic community. This paper’s goal is to perform an early (hopefully not too late) analysis to inform their development moving forward.},
	eventtitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	pages = {347--360},
	booktitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	author = {Sanchez Vicarte, Jose Rodrigo and Shome, Pradyumna and Nayak, Nandeeka and Trippel, Caroline and Morrison, Adam and Kohlbrenner, David and Fletcher, Christopher W.},
	date = {2021-06},
	note = {{ISSN}: 2575-713X},
	keywords = {Computer architecture, Microarchitecture, Prefetching, Privacy, Program processors, Programming, Systematics},
}

@inproceedings{kaseridis_minimalist_2011,
	title = {Minimalist open-page: A {DRAM} page-mode scheduling policy for the many-core era},
	url = {https://ieeexplore.ieee.org/document/7851456},
	shorttitle = {Minimalist open-page},
	abstract = {Contemporary {DRAM} systems have maintained impressive scaling by managing a careful balance between performance, power, and storage density. In achieving these goals, a significant sacrifice has been made in {DRAM}'s operational complexity. To realize good performance, systems must properly manage the significant number of structural and timing restrictions of the {DRAM} devices. {DRAM}'s use is further complicated in many-core systems where the memory interface is shared among multiple cores/threads competing for memory bandwidth. The use of the “Page-mode” feature of {DRAM} devices can mitigate many {DRAM} constraints. Current open-page policies attempt to garner the highest level of page hits. In an effort to achieve this, such greedy schemes map sequential address sequences to a single {DRAM} resource. This nonuniform resource usage pattern introduces high levels of conflict when multiple workloads in a many-core system map to the same set of resources. In this paper we present a scheme that provides a careful balance between the benefits (increased performance and decreased power), and the detractors (unfairness) of pagemode accesses. In our Minimalist approach, we target “just enough” page-mode accesses to garner page-mode benefits, avoiding system unfairness. We use a fair memory hashing scheme to control the maximum number of page mode hits, and direct the memory scheduler with processor-generated prefetch meta-data.},
	eventtitle = {2011 44th Annual {IEEE}/{ACM} International Symposium on Microarchitecture ({MICRO})},
	pages = {24--35},
	booktitle = {2011 44th Annual {IEEE}/{ACM} International Symposium on Microarchitecture ({MICRO})},
	author = {Kaseridis, Dimitris and Stuecheli, Jeffrey and John, Lizy Kurian},
	date = {2011-12},
	keywords = {Arrays, Complexity theory, Delays, Design, Performance, Prefetching, Random access memory},
}

@article{agarwal_leakage_2006,
	title = {Leakage Power Analysis and Reduction for Nanoscale Circuits},
	volume = {26},
	issn = {1937-4143},
	url = {https://ieeexplore.ieee.org/document/1624329},
	doi = {10.1109/MM.2006.39},
	abstract = {Leakage current in the nanometer regime has become a significant portion of power dissipation in {CMOS} circuits as threshold voltage, channel length, and gate oxide thickness scale downward. Various techniques are available to reduce leakage power in high-performance systems},
	pages = {68--80},
	number = {2},
	journaltitle = {{IEEE} Micro},
	author = {Agarwal, A. and Mukhopadhyay, S. and Raychowdhury, A. and Roy, K. and Kim, C.H.},
	date = {2006-03},
	note = {Conference Name: {IEEE} Micro},
	keywords = {{CMOS}, {CMOS} technology, Circuits, Doping profiles, Energy consumption, Leakage current, Logic design, Maintenance, Nanoscale devices, Subthreshold current, Thickness control, leakage power reduction, nanoscale circuits, technology scaling},
}

@inproceedings{jimenez_insertion_2013,
	title = {Insertion and promotion for tree-based {PseudoLRU} last-level caches},
	url = {https://ieeexplore.ieee.org/document/7847633},
	abstract = {Last-level caches mitigate the high latency of main memory. A good cache replacement policy enables high performance for memory intensive programs. To be useful to industry, a cache replacement policy must deliver high performance without high complexity or cost. For instance, the costly least-recently-used ({LRU}) replacement policy is not used in highly associative caches; rather, inexpensive policies with similar performance such as {PseudoLRU} are used. We propose a novel last-level cache replacement algorithm with approximately the same complexity and storage requirements as tree-based {PseudoLRU}, but with performance matching state of the art techniques such as dynamic re-reference interval prediction ({DRRIP}) and protecting distance policy ({PDP}). The algorithm is based on {PseudoLRU}, but uses set-dueling to dynamically adapt its insertion and promotion policy. It has slightly less than one bit of overhead per cache block, compared with two or more bits per cache block for competing policies. In this paper, we give the motivation behind the algorithm in the context of {LRU} with improved placement and promotion, then develop this motivation into a {PseudoLRU}-based algorithm, and finally give a version using set-dueling to allow adaptivity to changing program behavior. We show that, with a 16-way set-associative 4MB last-level cache, our adaptive {PseudoLRU} insertion and promotion algorithm yields a geometric mean speedup of 5.6\% over true {LRU} over all the {SPEC} {CPU} 2006 benchmarks using far less overhead than {LRU} or other algorithms. On a memory-intensive subset of {SPEC}, the technique gives a geometric mean speedup of 15.6\%. We show that the performance is comparable to state-of-the-art replacement policies that consume more than twice the area of our technique.},
	eventtitle = {2013 46th Annual {IEEE}/{ACM} International Symposium on Microarchitecture ({MICRO})},
	pages = {284--296},
	booktitle = {2013 46th Annual {IEEE}/{ACM} International Symposium on Microarchitecture ({MICRO})},
	author = {Jiménez, Daniel A.},
	date = {2013-12},
	keywords = {Benchmark testing, Complexity theory, Context, Genetic algorithms, Heuristic algorithms, Prediction algorithms, Space exploration},
}

@article{doweck_inside_2017,
	title = {Inside 6th-Generation Intel Core: New Microarchitecture Code-Named Skylake},
	volume = {37},
	issn = {1937-4143},
	url = {https://ieeexplore.ieee.org/document/7924286},
	doi = {10.1109/MM.2017.38},
	shorttitle = {Inside 6th-Generation Intel Core},
	abstract = {Skylake's core, processor graphics, and system on chip were designed to meet a demanding set of requirements for a wide range of power-performance points. Its coherent fabric was designed to provide high-memory bandwidth from multiple memory sources. Skylake's power management, which includes Intel Speed Shift technology, was designed to provide the largest dynamic power range among prior Intel processors. The Intel Architecture core delivers higher power efficiency, higher frequency, and a wider dynamic power range, supporting smaller form factors. Skylake's Gen9 graphics provides new features designed to maximize energy efficiency and bring the best visual experience for gaming and media. Skylake offers a rich performance monitoring unit that enhances software developers' ability to optimize their applications.},
	pages = {52--62},
	number = {2},
	journaltitle = {{IEEE} Micro},
	author = {Doweck, Jack and Kao, Wen-Fu and Lu, Allen Kuan-yu and Mandelblat, Julius and Rahatekar, Anirudha and Rappoport, Lihu and Rotem, Efraim and Yasin, Ahmad and Yoaz, Adi},
	date = {2017-03},
	note = {Conference Name: {IEEE} Micro},
	keywords = {Bandwidth, Central Processing Unit, Dynamic range, {GPU}, Graphics, Graphics processing units, Intel Speed Shift, Microarchitecture, Performance evaluation, Ports (Computers), Skylake, Turbo, {eDRAM}, microarchitecture, performance measurements, performance monitoring, power management},
}

@inproceedings{ganguly_interplay_2019,
	title = {Interplay between Hardware Prefetcher and Page Eviction Policy in {CPU}-{GPU} Unified Virtual Memory},
	url = {https://ieeexplore.ieee.org/document/8980317},
	abstract = {Memory capacity in {GPGPUs} is a major challenge for data-intensive applications with their ever increasing memory requirement. To fit a workload into the limited {GPU} memory space, a programmer needs to manually divide the workload by tiling the working set and perform user-level data migration. To relieve the programmer from this burden, Unified Virtual Memory ({UVM}) was developed to support on-demand paging and migration, transparent to the user. It further takes care of the memory over-subscription issue by automatically performing page replacement in an oversubscribed {GPU} memory situation. However, we found that naïve handling of page faults can cause orders of magnitude slowdown in performance. Moreover, we observed that although prefetching of data from {CPU} to {GPU} can hide the page fault latency, the difference among various prefetching mechanisms can lead to drastically different performance results. To this end, we performed extensive experiments on {GeForceGTX} 1080ti {GPUs} with {PCI}-e 3.0 16× to discover that there exists an effective prefetch mechanism to enhance locality in {GPU} memory. However, as the {GPU} memory is filled to its capacity, such prefetching mechanism quickly proves to be counterproductive due to locality unaware eviction policy. This necessitates the design of new eviction policies that are aware of the hardware prefetcher semantics. We propose two new programmer-agnostic, locality-aware pre-eviction policies which leverage the mechanics of existing hardware prefetcher and thus incur no additional implementation and performance overhead. We demonstrate that combining the proposed tree-based pre-eviction policy with the hardware prefetcher provides an average of 93\% and 18.5\% performance speed-up compared to {LRU} based 4KB and 2MB page replacement strategies, respectively. We further examine the memory access pattern of {GPU} workloads under consideration to analyze the achieved performance speed-up.},
	eventtitle = {2019 {ACM}/{IEEE} 46th Annual International Symposium on Computer Architecture ({ISCA})},
	pages = {224--235},
	booktitle = {2019 {ACM}/{IEEE} 46th Annual International Symposium on Computer Architecture ({ISCA})},
	author = {Ganguly, Debashis and Zhang, Ziyu and Yang, Jun and Melhem, Rami},
	date = {2019-06},
	note = {{ISSN}: 2575-713X},
	keywords = {{GPU}, hardware prefetcher, page eviction policy, unified virtual memory},
}

@inproceedings{vavouliotis_exploiting_2021,
	title = {Exploiting Page Table Locality for Agile {TLB} Prefetching},
	url = {https://ieeexplore.ieee.org/document/9499825},
	doi = {10.1109/ISCA52012.2021.00016},
	abstract = {Frequent Translation Lookaside Buffer ({TLB}) misses incur high performance and energy costs due to page walks required for fetching the corresponding address translations. Prefetching page table entries ({PTEs}) ahead of demand {TLB} accesses can mitigate the address translation performance bottleneck, but each prefetch requires traversing the page table, triggering additional accesses to the memory hierarchy. Therefore, {TLB} prefetching is a costly technique that may undermine performance when the prefetches are not accurate.In this paper we exploit the locality in the last level of the page table to reduce the cost and enhance the effectiveness of {TLB} prefetching by fetching cache-line adjacent {PTEs} "for free". We propose Sampling-Based Free {TLB} Prefetching ({SBFP}), a dynamic scheme that predicts the usefulness of these "free" {PTEs} and prefetches only the ones most likely to prevent {TLB} misses. We demonstrate that combining {SBFP} with novel and state-of-the-art {TLB} prefetchers significantly improves miss coverage and reduces most memory accesses due to page walks.Moreover, we propose Agile {TLB} Prefetcher ({ATP}), a novel composite {TLB} prefetcher particularly designed to maximize the benefits of {SBFP}. {ATP} efficiently combines three low-cost {TLB} prefetchers and disables {TLB} prefetching for those execution phases that do not benefit from it. Unlike state-of-the-art {TLB} prefetchers that correlate patterns with only one feature (e.g., strides, {PC}, distances), {ATP} correlates patterns with multiple features and dynamically enables the most appropriate {TLB} prefetcher per {TLB} miss.To alleviate the address translation performance bottleneck, we propose a unified solution that combines {ATP} and {SBFP}. Across an extensive set of industrial workloads provided by Qualcomm, {ATP} coupled with {SBFP} improves geometric speedup by 16.2\%, and eliminates on average 37\% of the memory references due to page walks. Considering the {SPEC} {CPU} 2006 and {SPEC} {CPU} 2017 benchmark suites, {ATP} with {SBFP} increases geometric speedup by 11.1\%, and eliminates page walk memory references by 26\%. Applied to big data workloads ({GAP} suite, {XSBench}), {ATP} with {SBFP} yields a geometric speedup of 11.8\% while reducing page walk memory references by 5\%. Over the best state-of-the-art {TLB} prefetcher for each benchmark suite, {ATP} with {SBFP} achieves speedups of 8.7\%, 3.4\%, and 4.2\% for the Qualcomm, {SPEC}, and {GAP}+{XSBench} workloads, respectively.},
	eventtitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	pages = {85--98},
	booktitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	author = {Vavouliotis, Georgios and Alvarez, Lluc and Karakostas, Vasileios and Nikas, Konstantinos and Koziris, Nectarios and Jiménez, Daniel A. and Casas, Marc},
	date = {2021-06},
	note = {{ISSN}: 2575-713X},
	keywords = {Benchmark testing, Big Data, Memory management, Prefetching, address translation, page table locality, prefetching, translation lookaside buffer, virtual memory},
}

@inproceedings{choi_fine-grained_2004,
	title = {Fine-grained dynamic voltage and frequency scaling for precise energy and performance trade-off based on the ratio of off-chip access to on-chip computation times},
	volume = {1},
	url = {https://ieeexplore.ieee.org/document/1268819},
	doi = {10.1109/DATE.2004.1268819},
	abstract = {This paper presents an intra-process dynamic voltage and frequency scaling ({DVFS}) technique targeted toward non real-time applications running on an embedded system platform. The key idea is to make use of runtime information about the external memory access statistics in order to perform {CPU} voltage and frequency scaling with the goal of minimizing the energy consumption while translucently controlling the performance penalty. The proposed {DVFS} technique relies on dynamically-constructed regression models that allow the {CPU} to calculate the expected workload and slack time for the next time slot, and thus, adjust its voltage and frequency in order to save energy while meeting soft timing constraints. This is in turn achieved by estimating and exploiting the ratio of the total off-chip access time to the total on-chip computation time. The proposed technique has been implemented on an {XScale}-based embedded system platform and actual energy savings have been calculated by current measurements in hardware. For memory-bound programs, a {CPU} energy saving of more than 70\% with a performance degradation of 12\% was achieved. For {CPU}-bound programs, 15/spl sim/60\% {CPU} energy saving was achieved at the cost of 5-20\% performance penalty.},
	eventtitle = {Automation and Test in Europe Conference and Exhibition Proceedings Design},
	pages = {4--9 Vol.1},
	booktitle = {Automation and Test in Europe Conference and Exhibition Proceedings Design},
	author = {Choi, Kihwan and Soma, R. and Pedram, M.},
	date = {2004-02},
	note = {{ISSN}: 1530-1591},
	keywords = {Current measurement, Dynamic voltage scaling, Embedded system, Energy consumption, Frequency, Real time systems, Runtime, Statistics, Timing, Voltage control},
}

@article{bhati_dram_2016,
	title = {{DRAM} Refresh Mechanisms, Penalties, and Trade-Offs},
	volume = {65},
	issn = {1557-9956},
	url = {https://ieeexplore.ieee.org/document/7070756},
	doi = {10.1109/TC.2015.2417540},
	abstract = {Ever-growing application data footprints demand faster main memory with larger capacity. {DRAM} has been the technology choice for main memory due to its low latency and high density. However, {DRAM} cells must be refreshed periodically to preserve their content. Refresh operations negatively affect performance and power. Traditionally, the performance and power overhead of refresh have been insignificant. But as the size and speed of {DRAM} chips continue to increase, refresh becomes a dominating factor of {DRAM} performance and power dissipation. In this paper, we conduct a comprehensive study of the issues related to refresh operations in modern {DRAMs}. Specifically, we describe the difference in refresh operations between modern synchronous {DRAM} and traditional asynchronous {DRAM}; the refresh modes and timings; and variations in data retention time. Moreover, we quantify refresh penalties versus device speed, size, and total memory capacity. We also categorize refresh mechanisms based on command granularity, and summarize refresh techniques proposed in research papers. Finally, based on our experiments and observations, we propose guidelines for mitigating {DRAM} refresh penalties.},
	pages = {108--121},
	number = {1},
	journaltitle = {{IEEE} Transactions on Computers},
	author = {Bhati, Ishwar and Chang, Mu-Tien and Chishti, Zeshan and Lu, Shih-Lien and Jacob, Bruce},
	date = {2016-01},
	note = {Conference Name: {IEEE} Transactions on Computers},
	keywords = {Computer architecture, {DRAM} Refresh, Multicore processor, Performance evaluation, {SDRAM}, Temperature sensors, Timing, performance, power},
}

@inproceedings{bakhshalipour_bingo_2019,
	title = {Bingo Spatial Data Prefetcher},
	url = {https://ieeexplore.ieee.org/document/8675188},
	doi = {10.1109/HPCA.2019.00053},
	abstract = {Applications extensively use data objects with a regular and fixed layout, which leads to the recurrence of access patterns over memory regions. Spatial data prefetching techniques exploit this phenomenon to prefetch future memory references and hide the long latency of {DRAM} accesses. While state-of-the-art spatial data prefetchers are effective at reducing the number of data misses, we observe that there is still significant room for improvement. To select an access pattern for prefetching, existing spatial prefetchers associate observed access patterns to either a short event with a high probability of recurrence or a long event with a low probability of recurrence. Consequently, the prefetchers either offer low accuracy or lose significant prediction opportunities. We identify that associating the observed spatial patterns to just a single event significantly limits the effectiveness of spatial data prefetchers. In this paper, we make a case for associating the observed spatial patterns to both short and long events to achieve high accuracy while not losing prediction opportunities. We propose Bingo spatial data prefetcher in which short and long events are used to select the best access pattern for prefetching. We propose a storage-efficient design for Bingo in such a way that just one history table is needed to maintain the association between the access patterns and the long and short events. Through a detailed evaluation of a set of big-data applications, we show that Bingo improves system performance by 60\% over a baseline with no data prefetcher and 11\% over the best-performing prior spatial data prefetcher.},
	eventtitle = {2019 {IEEE} International Symposium on High Performance Computer Architecture ({HPCA})},
	pages = {399--411},
	booktitle = {2019 {IEEE} International Symposium on High Performance Computer Architecture ({HPCA})},
	author = {Bakhshalipour, Mohammad and Shakerinava, Mehran and Lotfi-Kamran, Pejman and Sarbazi-Azad, Hamid},
	date = {2019-02},
	note = {{ISSN}: 2378-203X},
	keywords = {Big-Data Applications, Data Prefetching, Hardware, History, Memory System, Metadata, Prefetching, Random access memory, Spatial Correlation, Spatial databases, System performance},
}

@inproceedings{ren_i_2021,
	title = {I See Dead µops: Leaking Secrets via Intel/{AMD} Micro-Op Caches},
	url = {https://ieeexplore.ieee.org/document/9499837},
	doi = {10.1109/ISCA52012.2021.00036},
	shorttitle = {I See Dead µops},
	abstract = {Modern Intel, {AMD}, and {ARM} processors translate complex instructions into simpler internal micro-ops that are then cached in a dedicated on-chip structure called the micro-op cache. This work presents an in-depth characterization study of the micro-op cache, reverse-engineering many undocumented features, and further describes attacks that exploit the micro-op cache as a timing channel to transmit secret information. In particular, this paper describes three attacks – (1) a same thread cross-domain attack that leaks secrets across the user-kernel boundary, (2) a cross-{SMT} thread attack that transmits secrets across two {SMT} threads via the micro-op cache, and (3) transient execution attacks that have the ability to leak an unauthorized secret accessed along a misspeculated path, even before the transient instruction is dispatched to execution, breaking several existing invisible speculation and fencing-based solutions that mitigate Spectre.},
	eventtitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	pages = {361--374},
	booktitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	author = {Ren, Xida and Moody, Logan and Taram, Mohammadkazem and Jordan, Matthew and Tullsen, Dean M. and Venkat, Ashish},
	date = {2021-06},
	note = {{ISSN}: 2575-713X},
	keywords = {Computer architecture, Microarchitecture, Program processors, System-on-chip, Timing, Transient analysis},
}

@unpublished{intel_corporation_intel_2021,
	title = {Intel® 64 and {IA}-32 Architectures Software Developer’s Manual Combined Volumes: 1, 2A, 2B, 2C, 2D, 3A, 3B, 3C, 3D, and 4},
	url = {https://software.intel.com/content/www/us/en/develop/download/intel-64-and-ia-32-architectures-sdm-combined-volumes-1-2a-2b-2c-2d-3a-3b-3c-3d-and-4.html},
	shorttitle = {Intel® Software Developer’s Manual},
	abstract = {This document contains the following:

Volume 1: Describes the architecture and programming environment of processors supporting {IA}-32 and Intel® 64 architectures.

Volume 2: Includes the full instruction set reference, A-Z. Describes the format of the instruction and provides reference pages for instructions.

Volume 3: Includes the full system programming guide, parts 1, 2, 3, and 4. Describes the operating-system support environment of Intel® 64 and {IA}-32 architectures, including: memory management, protection, task management, interrupt and exception handling, multi-processor support, thermal and power management features, debugging, performance monitoring, system management mode, virtual machine extensions ({VMX}) instructions, Intel® Virtualization Technology (Intel® {VT}), and Intel® Software Guard Extensions (Intel® {SGX}).

Volume 4: Describes the model-specific registers of processors supporting {IA}-32 and Intel® 64 architectures.},
	pagetotal = {4778},
	author = {{Intel Corporation}},
	date = {2021-06-28},
}

@inproceedings{sethia_apogee_2013,
	title = {{APOGEE}: Adaptive prefetching on {GPUs} for energy efficiency},
	url = {https://ieeexplore.ieee.org/document/6618805},
	doi = {10.1109/PACT.2013.6618805},
	shorttitle = {{APOGEE}},
	abstract = {Modern graphics processing units ({GPUs}) combine large amounts of parallel hardware with fast context switching among thousands of active threads to achieve high performance. However, such designs do not translate well to mobile environments where power constraints often limit the amount of hardware. In this work, we investigate the use of prefetching as a means to increase the energy efficiency of {GPUs}. Classically, {CPU} prefetching results in higher performance but worse energy efficiency due to unnecessary data being brought on chip. Our approach, called {APOGEE}, uses an adaptive mechanism to dynamically detect and adapt to the memory access patterns found in both graphics and scientific applications that are run on modern {GPUs} to achieve prefetching efficiencies of over 90\%. Rather than examining threads in isolation, {APOGEE} uses adjacent threads to more efficiently identify address patterns and dynamically adapt the timeliness of prefetching. The net effect of {APOGEE} is that fewer thread contexts are necessary to hide memory latency and thus sustain performance. This reduction in thread contexts and related hardware translates to simplification of hardware and leads to a reduction in power. For Graphics and {GPGPU} applications, {APOGEE} enables an 8X reduction in multi-threading hardware, while providing a performance benefit of 19\%. This translates to a 52\% increase in performance per watt over systems with high multi-threading and 33\% over existing {GPU} prefetching techniques.},
	eventtitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
	pages = {73--82},
	booktitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
	author = {Sethia, Ankit and Dasika, Ganesh and Samadi, Mehrzad and Mahlke, Scott},
	date = {2013-09},
	note = {{ISSN}: 1089-795X},
	keywords = {Energy Efficiency, {GPU}, Graphics processing units, Prefetching, Throughput Processing},
}

@inproceedings{wu_dynamic_2005,
	title = {A dynamic compilation framework for controlling microprocessor energy and performance},
	url = {https://ieeexplore.ieee.org/document/1540966},
	doi = {10.1109/MICRO.2005.7},
	abstract = {Dynamic voltage and frequency scaling ({DVFS}) is an effective technique for controlling microprocessor energy and performance. Existing {DVFS} techniques are primarily based on hardware, {OS} time-interrupts, or static-compiler techniques. However, substantially greater gains can be realized when control opportunities are also explored in a dynamic compilation environment. There are several advantages to deploying {DVFS} and managing energy/performance tradeoffs through the use of a dynamic compiler. Most importantly, dynamic compiler driven {DVFS} is fine-grained, code-aware, and adaptive to the current microarchitecture environment. This paper presents a design framework of the run-time {DVFS} optimizer in a general dynamic compilation system. A prototype of the {DVFS} optimizer is implemented and integrated into an industrial-strength dynamic compilation system. The obtained optimization system is deployed in a real hardware platform that directly measures {CPU} voltage and current for accurate power and energy readings. Experimental results, based on physical measurements for over 40 {SPEC} or Olden benchmarks, show that significant energy savings are achieved with little performance degradation. {SPEC}2K {FP} benchmarks benefit with energy savings of up to 70\% (with 0.5\% performance loss). In addition, {SPEC}2K {INT} show up to 44\% energy savings (with 5\% performance loss), {SPEC}95 {FP} save up to 64\% (with 4.9\% performance loss), and Olden save up to 61\% (with 4.5\% performance loss). On average, the technique leads to an energy delay product ({EDP}) improvement that is 3times-5times better than static voltage scaling, and is more than 2times (22\% vs. 9\%) better than the reported {DVFS} results of prior static compiler work. While the proposed technique is an effective method for microprocessor voltage and frequency control, the design framework and methodology described in this paper have broader potential to address other energy and power issues such as di/dt and thermal control},
	eventtitle = {38th Annual {IEEE}/{ACM} International Symposium on Microarchitecture ({MICRO}'05)},
	pages = {12 pp.--282},
	booktitle = {38th Annual {IEEE}/{ACM} International Symposium on Microarchitecture ({MICRO}'05)},
	author = {Wu, Qiang and Reddi, V.J. and Wu, Youfeng and Lee, Jin and Connors, D. and Brooks, D. and Martonosi, M. and Clark, D.W.},
	date = {2005-11},
	note = {{ISSN}: 2379-3155},
	keywords = {Dynamic compiler, Dynamic voltage scaling, Energy management, Energy measurement, Frequency, Hardware, Microarchitecture, Microprocessors, Performance loss, Voltage control},
}

@inproceedings{ros_cost-effective_2021,
	title = {A Cost-Effective Entangling Prefetcher for Instructions},
	url = {https://ieeexplore.ieee.org/document/9499798},
	doi = {10.1109/ISCA52012.2021.00017},
	abstract = {Prefetching instructions in the instruction cache is a fundamental technique for designing high-performance computers. There are three key properties to consider when designing an efficient and effective prefetcher: timeliness, coverage, and accuracy. Timeliness is essential, as bringing instructions too early increases the risk of the instructions being evicted from the cache before their use and requesting them too late can lead to the instructions arriving after they are demanded. Coverage is important to reduce the number of instruction cache misses and accuracy to ensure that the prefetcher does not pollute the cache or interacts negatively with the other hardware mechanisms.This paper presents the Entangling Prefetcher for Instructions that entangles instructions to maximize timeliness. The prefetcher works by finding which instruction should trigger the prefetch for a subsequent instruction, accounting for the latency of each cache miss. The prefetcher is carefully adjusted to account for both coverage and accuracy. Our evaluation shows that with 40KB of storage, Entangling can increase performance up to 23\%, outperforming state-of-the-art prefetchers.},
	eventtitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	pages = {99--111},
	booktitle = {2021 {ACM}/{IEEE} 48th Annual International Symposium on Computer Architecture ({ISCA})},
	author = {Ros, Alberto and Jimborean, Alexandra},
	date = {2021-06},
	note = {{ISSN}: 2575-713X},
	keywords = {Computer architecture, Computers, Current measurement, Encoding, Hardware, Instruction prefetching, Organizations, Prefetching, caches, correlation, entangling, latency},
}

@online{an_chapter_nodate,
	title = {Chapter 2. Memory Addressing - Shichao's Notes},
	url = {https://notes.shichao.io/utlk/ch2/},
	abstract = {This chapter discusses addressing techniques by offering details in 80x86 microprocessors address memory chips and how Linux uses the available addressing circuits.},
	titleaddon = {Chapter 2. Memory Addressing - Shichao's Notes},
	type = {Personal Blog},
	author = {An, Shichao},
	urldate = {2021-09-25},
}

@software{utah_arch_research_group_usimm_2012,
	location = {Salt Lake City, {UT}},
	title = {{USIMM} Simulator},
	url = {http://www.cs.utah.edu/~rajeev/usimm-v1.3.tar.gz},
	shorttitle = {{USIMMSoftware}},
	version = {1.3},
	author = {{Utah Arch Research Group}},
	date = {2012-04-17},
}

@report{chatterjee_usimm_2012,
	location = {Portland, {OR}},
	title = {{USIMM}: the Utah {SImulated} Memory Module},
	url = {https://www.cs.utah.edu/%7Erajeev/pubs/usimm.pdf},
	shorttitle = {{USIMM}},
	abstract = {{USIMM}, the Utah {SImulated} Memory Module, is a {DRAM} main memory system simulator that is being released for use in the Memory Scheduling Championship ({MSC}), organized in conjunction with {ISCA}-39. {MSC} is part of the {JILP} Workshops on Computer Architecture Competitions ({JWAC}). This report describes the simulation infrastructure and how it will be used within the competition.},
	number = {{UUCS}-12-002},
	author = {Chatterjee, Niladrish and Balasubramonian, Rajeev and Shevgoor, Manjunath and Pugsley, Seth and Udipi, Aniruddha and Shafiee, Ali and Sudan, Kshitij and Awasthi, Manu and Chishti, Zeshan},
	date = {2012-02-20},
}

@inproceedings{farooq_store-load-branch_2013,
	title = {Store-Load-Branch ({SLB}) predictor: A compiler assisted branch prediction for data dependent branches},
	doi = {10.1109/HPCA.2013.6522307},
	shorttitle = {Store-Load-Branch ({SLB}) predictor},
	abstract = {Data-dependent branches constitute single biggest source of remaining branch mispredictions. Typically, data-dependent branches are associated with program data structures, and follow store-load-branch execution sequence. A set of memory locations is written at an earlier point in a program. Later, these locations are read, and used for evaluating branch condition. Branch outcome depends on data values stored in data structure, which, typically do not have repeatable pattern. Therefore, in addition to history-based dynamic predictor, we need a different kind of predictor for handling such branches. This paper presents Store-Load-Branch ({SLB}) predictor; a compiler-assisted dynamic branch prediction scheme for data-dependent direct and indirect branches. For every data-dependent branch, compiler identifies store instructions that modify the data structure associated with the branch. Marked store instructions are dynamically tracked, and stored values are used for computing branch flags ahead of time. Branch flags are buffered, and later used for making predictions. On average, compared to standalone {TAGE} predictor, combined {TAGE}+{SLB} predictor reduces branch {MPKI} by 21\% and 51\% for {SPECINT} and {EEMBC} benchmark suites respectively.},
	eventtitle = {2013 {IEEE} 19th International Symposium on High Performance Computer Architecture ({HPCA})},
	pages = {59--70},
	booktitle = {2013 {IEEE} 19th International Symposium on High Performance Computer Architecture ({HPCA})},
	author = {Farooq, M. Umar and {Khubaib} and John, Lizy K.},
	date = {2013-02},
	note = {{ISSN}: 1530-0897},
	keywords = {Accuracy, Arrays, Benchmark testing, Discrete cosine transforms, Hardware, History},
}

@inproceedings{lin_branch_2019,
	title = {Branch Prediction Is Not A Solved Problem: Measurements, Opportunities, and Future Directions},
	doi = {10.1109/IISWC47752.2019.9042108},
	shorttitle = {Branch Prediction Is Not A Solved Problem},
	abstract = {Modern branch predictors predict the vast majority of conditional branch instructions with near-perfect accuracy, allowing superscalar, out-of-order processors to maximize speculative efficiency and thus performance. However, this impressive overall effectiveness belies a substantial missed opportunity in single-threaded instructions per cycle ({IPC}). For example, we show that correcting the mispredictions made by the state-of-the-art {TAGE}-{SC}-L branch predictor on {SPECint} 2017 would improve {IPC} by margins similar to an advance in process technology node. In this work, we measure and characterize these mispredictions. We find that they categorically arise from either (1) a small number of systematically hard-to-predict (H2P) branches; or (2) rare branches with low dynamic execution counts. Using data from {SPECint} 2017 and additional large code footprint applications, we quantify the occurrence and {IPC} impact of these two categories. We then demonstrate that solely increasing the resources afforded to existing branch predictors does not address the root causes of most mispredictions. This leads us to reexamine basic assumptions in branch prediction and to propose new research directions that, for example, deploy machine learning to improve pattern matching for H2Ps, and use on-chip phase learning to track long-term statistics for rare branches.},
	eventtitle = {2019 {IEEE} International Symposium on Workload Characterization ({IISWC})},
	pages = {228--238},
	booktitle = {2019 {IEEE} International Symposium on Workload Characterization ({IISWC})},
	author = {Lin, Chit-Kwan and Tarsa, Stephen J.},
	date = {2019-11},
}

@article{mittal_survey_2018,
	title = {A Survey of Techniques for Dynamic Branch Prediction},
	url = {http://arxiv.org/abs/1804.00261},
	abstract = {Branch predictor ({BP}) is an essential component in modern processors since high {BP} accuracy can improve performance and reduce energy by decreasing the number of instructions executed on wrong-path. However, reducing latency and storage overhead of {BP} while maintaining high accuracy presents significant challenges. In this paper, we present a survey of dynamic branch prediction techniques. We classify the works based on key features to underscore their differences and similarities. We believe this paper will spark further research in this area and will be useful for computer architects, processor designers and researchers.},
	journaltitle = {{arXiv}:1804.00261 [cs]},
	author = {Mittal, Sparsh},
	urldate = {2021-09-24},
	date = {2018-04-01},
	eprinttype = {arxiv},
	eprint = {1804.00261},
	keywords = {Computer Science - Hardware Architecture},
}

@inproceedings{skarlatos_elastic_2020,
	location = {New York, {NY}, {USA}},
	title = {Elastic Cuckoo Page Tables: Rethinking Virtual Memory Translation for Parallelism},
	isbn = {978-1-4503-7102-5},
	url = {https://doi.org/10.1145/3373376.3378493},
	doi = {10.1145/3373376.3378493},
	series = {{ASPLOS} '20},
	shorttitle = {Elastic Cuckoo Page Tables},
	abstract = {The unprecedented growth in the memory needs of emerging memory-intensive workloads has made virtual memory translation a major performance bottleneck. To address this problem, this paper introduces Elastic Cuckoo Page Tables, a novel page table design that transforms the sequential pointer-chasing operation used by conventional multi-level radix page tables into fully-parallel look-ups. The resulting design harvests, for the first time, the benefits of memory level parallelism for address translation. Elastic cuckoo page tables use Elastic Cuckoo Hashing, a novel extension of cuckoo hashing that supports efficient page table resizing. Elastic cuckoo page tables efficiently resolve hash collisions, provide process-private page tables, support multiple page sizes and page sharing among processes, and dynamically adapt page table sizes to meet application requirements. We evaluate elastic cuckoo page tables with full-system simulations of an 8-core processor using a set of graph analytics, bioinformatics, {HPC}, and system workloads. Elastic cuckoo page tables reduce the address translation overhead by an average of 41\% over conventional radix page tables. The result is a 3-18\% speed-up in application execution.},
	pages = {1093--1108},
	booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {Association for Computing Machinery},
	author = {Skarlatos, Dimitrios and Kokolis, Apostolos and Xu, Tianyin and Torrellas, Josep},
	urldate = {2021-09-24},
	date = {2020-03-09},
	keywords = {cuckoo hashing, page tables, virtual memory},
}

@inproceedings{parasar_seesaw_2018,
	location = {Los Angeles, California},
	title = {{SEESAW}: using superpages to improve {VIPT} caches},
	isbn = {978-1-5386-5984-7},
	url = {https://doi.org/10.1109/ISCA.2018.00026},
	doi = {10.1109/ISCA.2018.00026},
	series = {{ISCA} '18},
	shorttitle = {{SEESAW}},
	abstract = {Hardware caches balance fast lookup, high hit rates, energy efficiency, and simplicity of implementation. For L1 caches however, achieving this balance is difficult because of constraints imposed by virtual memory. L1 caches are usually virtually-indexed and physically tagged ({VIPT}), but this means that they must be highly associative to achieve good capacity. Unfortunately, excessive associativity compromises performance by degrading access times without significantly boosting hit rates, and increases access energy. We propose Seesaw to overcome this problem. Seesaw leverages the increasing ubiquity of superpages1 - since super-pages have more page offset bits, they can accommodate {VIPT} caches with more sets than what is traditionally possible with only base page sizes. Seesaw dynamically reduces the number of ways that are looked up based on the page size, improving performance and energy. Seesaw requires modest hardware and no {OS} or application changes.},
	pages = {193--206},
	booktitle = {Proceedings of the 45th Annual International Symposium on Computer Architecture},
	publisher = {{IEEE} Press},
	author = {Parasar, Mayank and Bhattacharjee, Abhishek and Krishna, Tushar},
	urldate = {2021-09-24},
	date = {2018-06-02},
	keywords = {L1 caches, memory systems, superpages, virtual memory},
}

@inproceedings{chang_improving_2014,
	title = {Improving {DRAM} performance by parallelizing refreshes with accesses},
	doi = {10.1109/HPCA.2014.6835946},
	abstract = {Modern {DRAM} cells are periodically refreshed to prevent data loss due to leakage. Commodity {DDR} (double data rate) {DRAM} refreshes cells at the rank level. This degrades performance significantly because it prevents an entire {DRAM} rank from serving memory requests while being refreshed. {DRAM} designed for mobile platforms, {LPDDR} (low power {DDR}) {DRAM}, supports an enhanced mode, called per-bank refresh, that refreshes cells at the bank level. This enables a bank to be accessed while another in the same rank is being refreshed, alleviating part of the negative performance impact of refreshes. Unfortunately, there are two shortcomings of per-bank refresh employed in today's systems. First, we observe that the perbank refresh scheduling scheme does not exploit the full potential of overlapping refreshes with accesses across banks because it restricts the banks to be refreshed in a sequential round-robin order. Second, accesses to a bank that is being refreshed have to wait. To mitigate the negative performance impact of {DRAM} refresh, we propose two complementary mechanisms, {DARP} (Dynamic Access Refresh Parallelization) and {SARP} (Subarray Access Refresh Parallelization). The goal is to address the drawbacks of per-bank refresh by building more efficient techniques to parallelize refreshes and accesses within {DRAM}. First, instead of issuing per-bank refreshes in a round-robin order, as it is done today, {DARP} issues per-bank refreshes to idle banks in an out-of-order manner. Furthermore, {DARP} proactively schedules refreshes during intervals when a batch of writes are draining to {DRAM}. Second, {SARP} exploits the existence of mostly-independent subarrays within a bank. With minor modifications to {DRAM} organization, it allows a bank to serve memory accesses to an idle subarray while another subarray is being refreshed. Extensive evaluations on a wide variety of workloads and systems show that our mechanisms improve system performance (and energy efficiency) compared to three state-of-the-art refresh policies and the performance benefit increases as {DRAM} density increases.},
	eventtitle = {2014 {IEEE} 20th International Symposium on High Performance Computer Architecture ({HPCA})},
	pages = {356--367},
	booktitle = {2014 {IEEE} 20th International Symposium on High Performance Computer Architecture ({HPCA})},
	author = {Chang, Kevin Kai-Wei and Lee, Donghyuk and Chishti, Zeshan and Alameldeen, Alaa R. and Wilkerson, Chris and Kim, Yoongu and Mutlu, Onur},
	date = {2014-02},
	note = {{ISSN}: 2378-203X},
	keywords = {{DRAM} chips, Organizations, Out of order, Schedules, Standards, System performance},
}

@inproceedings{liu_raidr_2012,
	title = {{RAIDR}: Retention-aware intelligent {DRAM} refresh},
	doi = {10.1109/ISCA.2012.6237001},
	shorttitle = {{RAIDR}},
	abstract = {Dynamic random-access memory ({DRAM}) is the building block of modern main memory systems. {DRAM} cells must be periodically refreshed to prevent loss of data. These refresh operations waste energy and degrade system performance by interfering with memory accesses. The negative effects of {DRAM} refresh increase as {DRAM} device capacity increases. Existing {DRAM} devices refresh all cells at a rate determined by the leakiest cell in the device. However, most {DRAM} cells can retain data for significantly longer. Therefore, many of these refreshes are unnecessary. In this paper, we propose {RAIDR} (Retention-Aware Intelligent {DRAM} Refresh), a low-cost mechanism that can identify and skip unnecessary refreshes using knowledge of cell retention times. Our key idea is to group {DRAM} rows into retention time bins and apply a different refresh rate to each bin. As a result, rows containing leaky cells are refreshed as frequently as normal, while most rows are refreshed less frequently. {RAIDR} uses Bloom filters to efficiently implement retention time bins. {RAIDR} requires no modification to {DRAM} and minimal modification to the memory controller. In an 8-core system with 32 {GB} {DRAM}, {RAIDR} achieves a 74.6\% refresh reduction, an average {DRAM} power reduction of 16.1\%, and an average system performance improvement of 8.6\% over existing systems, at a modest storage overhead of 1.25 {KB} in the memory controller. {RAIDR}'s benefits are robust to variation in {DRAM} system configuration, and increase as memory capacity increases.},
	eventtitle = {2012 39th Annual International Symposium on Computer Architecture ({ISCA})},
	pages = {1--12},
	booktitle = {2012 39th Annual International Symposium on Computer Architecture ({ISCA})},
	author = {Liu, Jamie and Jaiyen, Ben and Veras, Richard and Mutlu, Onur},
	date = {2012-06},
	note = {{ISSN}: 1063-6897},
	keywords = {Arrays, Capacitors, Memory management, Performance evaluation, Radiation detectors, Random access memory, Throughput},
}

@inproceedings{shi_hierarchical_2021,
	location = {Virtual {USA}},
	title = {A hierarchical neural model of data prefetching},
	isbn = {978-1-4503-8317-2},
	url = {https://dl.acm.org/doi/10.1145/3445814.3446752},
	doi = {10.1145/3445814.3446752},
	eventtitle = {{ASPLOS} '21: 26th {ACM} International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {861--873},
	booktitle = {Proceedings of the 26th {ACM} International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {{ACM}},
	author = {Shi, Zhan and Jain, Akanksha and Swersky, Kevin and Hashemi, Milad and Ranganathan, Parthasarathy and Lin, Calvin},
	urldate = {2021-09-24},
	date = {2021-04-19},
	langid = {english},
}

@article{mittal_survey_2016,
	title = {A Survey of Recent Prefetching Techniques for Processor Caches},
	volume = {49},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/2907071},
	doi = {10.1145/2907071},
	abstract = {As the trends of process scaling make memory systems an even more crucial bottleneck, the importance of latency hiding techniques such as prefetching grows further. However, naively using prefetching can harm performance and energy efficiency and, hence, several factors and parameters need to be taken into account to fully realize its potential. In this article, we survey several recent techniques that aim to improve the implementation and effectiveness of prefetching. We characterize the techniques on several parameters to highlight their similarities and differences. The aim of this survey is to provide insights to researchers into working of prefetching techniques and spark interesting future work for improving the performance advantages of prefetching even further.},
	pages = {35:1--35:35},
	number = {2},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Mittal, Sparsh},
	urldate = {2021-09-24},
	date = {2016-08-02},
	keywords = {Review, cache pollution, classification, data prefetching, hardware ({HW}) prefetching, helper thread prefetching, instruction prefetching, software ({SW}) prefetching, speculative pre-execution},
}

@inproceedings{al-zoubi_performance_2004,
	location = {New York, {NY}, {USA}},
	title = {Performance evaluation of cache replacement policies for the {SPEC} {CPU}2000 benchmark suite},
	isbn = {978-1-58113-870-2},
	url = {https://doi.org/10.1145/986537.986601},
	doi = {10.1145/986537.986601},
	series = {{ACM}-{SE} 42},
	abstract = {Replacement policy, one of the key factors determining the effectiveness of a cache, becomes even more important with latest technological trends toward highly associative caches. The state-of-the-art processors employ various policies such as Random, Least Recently Used ({LRU}), Round-Robin, and {PLRU} (Pseudo {LRU}), indicating that there is no common wisdom about the best one. Optimal yet unattainable policy would replace cache memory block whose next reference is the farthest away in the future, among all memory blocks present in the set.In our quest for replacement policy as close to optimal as possible, we thoroughly explored the design space of existing replacement mechanisms using {SimpleScalar} toolset and {SPEC} {CPU}2000 benchmark suite, across wide range of cache sizes and organizations. In order to better understand the behavior of different policies, we introduced new measures, such as cumulative distribution of cache hits in the {LRU} stack. We also dynamically monitored the number of cache misses, per each 100000 instructions.Our results show that the {PLRU} techniques can approximate and even outperform {LRU} with much lower complexity, for a wide range of cache organizations. However, a relatively large gap between {LRU} and optimal replacement policy, of up to 50\%, indicates that new research aimed to close the gap is necessary. The cumulative distribution of cache hits in the {LRU} stack indicates a very good potential for way prediction using {LRU} information, since the percentage of hits to the bottom of the {LRU} stack is relatively high.},
	pages = {267--272},
	booktitle = {Proceedings of the 42nd annual Southeast regional conference},
	publisher = {Association for Computing Machinery},
	author = {Al-Zoubi, Hussein and Milenkovic, Aleksandar and Milenkovic, Milena},
	urldate = {2021-09-24},
	date = {2004-04-02},
	keywords = {cache memory, performance evaluation, replacement policy},
}

@inproceedings{sanchez_zcache_2010,
	title = {The {ZCache}: Decoupling Ways and Associativity},
	doi = {10.1109/MICRO.2010.20},
	shorttitle = {The {ZCache}},
	abstract = {The ever-increasing importance of main memory latency and bandwidth is pushing {CMPs} towards caches with higher capacity and associativity. Associativity is typically improved by increasing the number of ways. This reduces conflict misses, but increases hit latency and energy, placing a stringent trade-off on cache design. We present the zcache, a cache design that allows much higher associativity than the number of physical ways (e.g. a 64-associative cache with 4 ways). The zcache draws on previous research on skew-associative caches and cuckoo hashing. Hits, the common case, require a single lookup, incurring the latency and energy costs of a cache with a very low number of ways. On a miss, additional tag lookups happen off the critical path, yielding an arbitrarily large number of replacement candidates for the incoming block. Unlike conventional designs, the zcache provides associativity by increasing the number of replacement candidates, but not the number of cache ways. To understand the implications of this approach, we develop a general analysis framework that allows to compare associativity across different cache designs (e.g. a set-associative cache and a zcache) by representing associativity as a probability distribution. We use this framework to show that for zcaches, associativity depends only on the number of replacement candidates, and is independent of other factors (such as the number of cache ways or the workload). We also show that, for the same number of replacement candidates, the associativity of a zcache is superior than that of a set-associative cache for most workloads. Finally, we perform detailed simulations of multithreaded and multiprogrammed workloads on a large-scale {CMP} with zcache as the last-level cache. We show that zcaches provide higher performance and better energy efficiency than conventional caches without incurring the overheads of designs with a large number of ways.},
	eventtitle = {2010 43rd Annual {IEEE}/{ACM} International Symposium on Microarchitecture},
	pages = {187--198},
	booktitle = {2010 43rd Annual {IEEE}/{ACM} International Symposium on Microarchitecture},
	author = {Sanchez, Daniel and Kozyrakis, Christos},
	date = {2010-12},
	note = {{ISSN}: 2379-3155},
	keywords = {Arrays, Bandwidth, Delay, Indexes, Process control, Program processors, Radiation detectors, associativity, cache, energy efficiency, multi-core, performance},
}

@inproceedings{kedzierski_adapting_2010,
	title = {Adapting cache partitioning algorithms to pseudo-{LRU} replacement policies},
	doi = {10.1109/IPDPS.2010.5470352},
	abstract = {Recent studies have shown that cache partitioning is an efficient technique to improve throughput, fairness and Quality of Service ({QoS}) in {CMP} processors. The cache partitioning algorithms proposed so far assume Least Recently Used ({LRU}) as the underlying replacement policy. However, it has been shown that the true {LRU} imposes extraordinary complexity and area overheads when implemented on high associativity caches, such as last level caches. As a consequence, current processors available on the market use pseudo-{LRU} replacement policies, which provide similar behavior as {LRU}, while reducing the hardware complexity. Thus, the presented so far {LRU}-based cache partitioning solutions cannot be applied to real {CMP} architectures. This paper proposes a complete partitioning system for caches using the pseudo-{LRU} replacement policy. In particular, the paper focuses on the pseudo-{LRU} implementations proposed by Sun Microsystems and {IBM}, called Not Recently Used ({NRU}) and Binary Tree ({BT}), respectively. We propose a high accuracy profiling logic and a cache partitioning hardware for both schemes. We evaluate our proposals' hardware costs in terms of area and power, and compare them against the {LRU} partitioning algorithm. Overall, this paper presents two hardware techniques to adapt the existing cache partitioning algorithms to real replacement policies. The results show that our solutions impose negligible performance degradation with respect to the {LRU}.},
	eventtitle = {2010 {IEEE} International Symposium on Parallel Distributed Processing ({IPDPS})},
	pages = {1--12},
	booktitle = {2010 {IEEE} International Symposium on Parallel Distributed Processing ({IPDPS})},
	author = {Kędzierski, Kamil and Moreto, Miquel and Cazorla, Francisco J. and Valero, Mateo},
	date = {2010-04},
	note = {{ISSN}: 1530-2075},
	keywords = {Binary trees, {CMP}, Costs, Degradation, Hardware, Logic, Partitioning algorithms, Proposals, Pseudo-{LRU}, Quality of service, Shared last level cache, Sun, Throughput},
}

@inproceedings{jaleel_high_2010,
	location = {New York, {NY}, {USA}},
	title = {High performance cache replacement using re-reference interval prediction ({RRIP})},
	isbn = {978-1-4503-0053-7},
	url = {https://doi.org/10.1145/1815961.1815971},
	doi = {10.1145/1815961.1815971},
	series = {{ISCA} '10},
	abstract = {Practical cache replacement policies attempt to emulate optimal replacement by predicting the re-reference interval of a cache block. The commonly used {LRU} replacement policy always predicts a near-immediate re-reference interval on cache hits and misses. Applications that exhibit a distant re-reference interval perform badly under {LRU}. Such applications usually have a working-set larger than the cache or have frequent bursts of references to non-temporal data (called scans). To improve the performance of such workloads, this paper proposes cache replacement using Re-reference Interval Prediction ({RRIP}). We propose Static {RRIP} ({SRRIP}) that is scan-resistant and Dynamic {RRIP} ({DRRIP}) that is both scan-resistant and thrash-resistant. Both {RRIP} policies require only 2-bits per cache block and easily integrate into existing {LRU} approximations found in modern processors. Our evaluations using {PC} games, multimedia, server and {SPEC} {CPU}2006 workloads on a single-core processor with a 2MB last-level cache ({LLC}) show that both {SRRIP} and {DRRIP} outperform {LRU} replacement on the throughput metric by an average of 4\% and 10\% respectively. Our evaluations with over 1000 multi-programmed workloads on a 4-core {CMP} with an 8MB shared {LLC} show that {SRRIP} and {DRRIP} outperform {LRU} replacement on the throughput metric by an average of 7\% and 9\% respectively. We also show that {RRIP} outperforms {LFU}, the state-of the art scan-resistant replacement algorithm to-date. For the cache configurations under study, {RRIP} requires 2X less hardware than {LRU} and 2.5X less hardware than {LFU}.},
	pages = {60--71},
	booktitle = {Proceedings of the 37th annual international symposium on Computer architecture},
	publisher = {Association for Computing Machinery},
	author = {Jaleel, Aamer and Theobald, Kevin B. and Steely, Simon C. and Emer, Joel},
	urldate = {2021-09-24},
	date = {2010-06-19},
	keywords = {replacement, scan resistance, shared cache, thrashing},
}

@inproceedings{tang_clkscrew_2017,
	location = {Vancouver, {BC}},
	title = {{CLKSCREW}: Exposing the Perils of Security-Oblivious Energy Management},
	isbn = {978-1-931971-40-9},
	url = {https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/tang},
	shorttitle = {{CLKscrew}},
	abstract = {The need for power- and energy-efficient computing has resulted in aggressive cooperative hardware-software energy management mechanisms on modern commodity devices. Most systems today, for example, allow software to control the frequency and voltage of the underlying hardware at a very fine granularity to extend battery life. Despite their benefits, these software-exposed energy management mechanisms pose grave security implications that have not been studied before.

In this work, we present the {CLKSCREW} attack, a new class of fault attacks that exploit the security-obliviousness of energy management mechanisms to break security. A novel benefit for the attackers is that these fault attacks become more accessible since they can now be conducted without the need for physical access to the devices or fault injection equipment. We demonstrate {CLKSCREW} on commodity {ARM}/Android devices. We show that a malicious kernel driver (1) can extract secret cryptographic keys from Trustzone, and (2) can escalate its privileges by loading self-signed code into Trustzone. As the first work to show the security ramifications of energy management mechanisms, we urge the community to re-examine these security-oblivious designs.},
	eventtitle = {26th {USENIX} Security Symposium ({USENIX} Security 17)},
	pages = {1057--1074},
	publisher = {{USENIX} Association},
	author = {Tang, Andrew and Sethumadhavan, Simha and Stolfo, Salvatore},
	date = {2017-08},
}

@inproceedings{amit_optimizing_2017,
	location = {{USA}},
	title = {Optimizing the {TLB} shootdown algorithm with page access tracking},
	isbn = {978-1-931971-38-6},
	series = {{USENIX} {ATC} '17},
	abstract = {The operating system is tasked with maintaining the coherency of per-core {TLBs}, necessitating costly synchronization operations, notably to invalidate stale mappings. As core-counts increase, the overhead of {TLB} synchronization likewise increases and hinders scalability, whereas existing software optimizations that attempt to alleviate the problem (like batching) are lacking. We address this problem by revising the {TLB} synchronization subsystem. We introduce several techniques that detect cases whereby soon-tobe invalidated mappings are cached by only one {TLB} or not cached at all, allowing us to entirely avoid the cost of synchronization. In contrast to existing optimizations, our approach leverages hardware page access tracking. We implement our techniques in Linux and find that they reduce the number of {TLB} invalidations by up to 98\% on average and thus improve performance by up to 78\%. Evaluations show that while our techniques may introduce overheads of up to 9\% when memory mappings are never removed, these overheads can be avoided by simple hardware enhancements.},
	pages = {27--39},
	booktitle = {Proceedings of the 2017 {USENIX} Conference on Usenix Annual Technical Conference},
	publisher = {{USENIX} Association},
	author = {Amit, Nadav},
	urldate = {2021-09-23},
	date = {2017-07-12},
}

@report{mcfarling_combining_1993,
	title = {Combining Branch Predictors},
	url = {https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-TN-36.pdf},
	shorttitle = {{CombineBP}},
	abstract = {One of the key factors determining computer performance is the degree to which the implementation can take advantage of instruction-level parallelism. Perhaps the most critical limit to this parallelism is the presence of conditional branches that determine which instructions need to be executed next. To increase parallelism, several authors have suggested ways of predicting the direction of conditional branches with hardware that uses the history of previous branches. The different proposed predictors take advantage of different observed patterns in branch behavior. This paper presents a method of combining the advantages of these different types of predictors. The new method uses a history mechanism to keep track of which predictor is most accurate for each branch so that the most accurate predictor can be used. In addition, this paper describes a method of increasing the usefulness of branch history by hashing it together with the branch address. Together, these new techniques are shown to outperform previously known approaches both in terms of maximum prediction accuracy and the prediction accuracy for a given predictor size. Specifically, prediction accuracy reaches 98.1\% correct versus  97.1\% correct for the most accurate previously known approach. Also, this new approach is typically at least a factor of two smaller than other schemes for a given prediction accuracy. Finally, this new approach allows predictors with a single level of history array access to outperform schemes with multiple levels of history for all but the largest predictor sizes.},
	pages = {29},
	number = {{WRL}-{TN}-36},
	type = {Technical Note},
	author = {{McFarling}, Scott},
	date = {1993},
	keywords = {Computer architecture, branch predictor, instruction level parallelism},
}

@inproceedings{bhattacharjee_characterizing_2009,
	title = {Characterizing the {TLB} Behavior of Emerging Parallel Workloads on Chip Multiprocessors},
	doi = {10.1109/PACT.2009.26},
	abstract = {Translation Lookaside Buffers ({TLBs}) are a staple in modern computer systems and have a significant impact on overall system performance. Numerous prior studies have addressed {TLB} designs to lower access times and miss rates; these, however, have been targeted towards uniprocessor architectures. As the computer industry embraces chip multiprocessor ({CMP}) architectures, it is important to study the {TLB} behavior of emerging parallel workloads. This work presents the first full-system characterization of the {TLB} behavior of emerging parallel applications on real-system {CMPs}. Using the {PARSEC} benchmarks, representative of emerging {RMS} workloads, we show that {TLB} misses can hinder system performance significantly. We also evaluate {TLB} miss stream patterns and show that multiple threads of a parallel execution experience a large number of redundant and predictable misses. For our evaluated benchmarks, 30\% to 95\% of the total misses fall under this category. Our results point to the need for novel {TLB} designs encouraging inter-core cooperation, either through hierarchically shared {TLBs} or through inter-core {TLB} prediction mechanisms.},
	eventtitle = {2009 18th International Conference on Parallel Architectures and Compilation Techniques},
	pages = {29--40},
	booktitle = {2009 18th International Conference on Parallel Architectures and Compilation Techniques},
	author = {Bhattacharjee, Abhishek and Martonosi, Margaret},
	date = {2009-09},
	note = {{ISSN}: 1089-795X},
	keywords = {Chip Multiprocessor, Computer architecture, Computer industry, Concurrent computing, Delay, Hardware, Memory management, {PARSEC}, Parallel architectures, System performance, System-on-a-chip, Translation Lookaside Buffers, Yarn},
}

@inproceedings{baumann_multikernel_2009,
	location = {New York, {NY}, {USA}},
	title = {The multikernel: a new {OS} architecture for scalable multicore systems},
	isbn = {978-1-60558-752-3},
	url = {https://doi.org/10.1145/1629575.1629579},
	doi = {10.1145/1629575.1629579},
	series = {{SOSP} '09},
	shorttitle = {The multikernel},
	abstract = {Commodity computer systems contain more and more processor cores and exhibit increasingly diverse architectural tradeoffs, including memory hierarchies, interconnects, instruction sets and variants, and {IO} configurations. Previous high-performance computing systems have scaled in specific cases, but the dynamic nature of modern client and server workloads, coupled with the impossibility of statically optimizing an {OS} for all workloads and hardware variants pose serious challenges for operating system structures. We argue that the challenge of future multicore hardware is best met by embracing the networked nature of the machine, rethinking {OS} architecture using ideas from distributed systems. We investigate a new {OS} structure, the multikernel, that treats the machine as a network of independent cores, assumes no inter-core sharing at the lowest level, and moves traditional {OS} functionality to a distributed system of processes that communicate via message-passing. We have implemented a multikernel {OS} to show that the approach is promising, and we describe how traditional scalability problems for operating systems (such as memory management) can be effectively recast using messages and can exploit insights from distributed systems and networking. An evaluation of our prototype on multicore systems shows that, even on present-day machines, the performance of a multikernel is comparable with a conventional {OS}, and can scale better to support future hardware.},
	pages = {29--44},
	booktitle = {Proceedings of the {ACM} {SIGOPS} 22nd symposium on Operating systems principles},
	publisher = {Association for Computing Machinery},
	author = {Baumann, Andrew and Barham, Paul and Dagand, Pierre-Evariste and Harris, Tim and Isaacs, Rebecca and Peter, Simon and Roscoe, Timothy and Schüpbach, Adrian and Singhania, Akhilesh},
	urldate = {2021-09-23},
	date = {2009-10-11},
	keywords = {message passing, multicore processors, scalability},
}

@inproceedings{villavieja_didi_2011,
	location = {Galveston, {TX}, {USA}},
	title = {{DiDi}: Mitigating the Performance Impact of {TLB} Shootdowns Using a Shared {TLB} Directory},
	isbn = {978-1-4577-1794-9 978-0-7695-4566-0},
	url = {http://ieeexplore.ieee.org/document/6113842/},
	doi = {10.1109/PACT.2011.65},
	shorttitle = {{DiDi}},
	abstract = {Translation Look aside Buffers ({TLBs}) are ubiquitously used in modern architectures to cache virtual-to-physical mappings and, as they are looked up on every memory access, are paramount to performance scalability. The emergence of chip-multiprocessors ({CMPs}) with per-core {TLBs}, has brought the problem of {TLB} coherence to front stage. {TLBs} are kept coherent at the software-level by the operating system ({OS}). Whenever the {OS} modifies page permissions in a page table, it must initiate a coherency transaction among {TLBs}, a process known as a {TLB} shoot down. Current {CMPs} rely on the {OS} to approximate the set of {TLBs} caching a mapping and synchronize {TLBs} using costly Inter-Proceessor Interrupts ({IPIs}) and software handlers. In this paper, we characterize the impact of {TLB} shoot downs on multiprocessor performance and scalability, and present the design of a scalable {TLB} coherency mechanism. First, we show that both {TLB} shoot down cost and frequency increase with the number of processors and project that software-based {TLB} shoot downs would thwart the performance of large multiprocessors. We then present a scalable architectural mechanism that couples a shared {TLB} directory with load/store queue support for lightweight {TLB} invalidation, and thereby eliminates the need for costly {IPIs}. Finally, we show that the proposed mechanism reduces the fraction of machine cycles wasted on {TLB} shoot downs by an order of magnitude.},
	eventtitle = {2011 International Conference on Parallel Architectures and Compilation Techniques ({PACT})},
	pages = {340--349},
	booktitle = {2011 International Conference on Parallel Architectures and Compilation Techniques},
	publisher = {{IEEE}},
	author = {Villavieja, Carlos and Karakostas, Vasileios and Vilanova, Lluis and Etsion, Yoav and Ramirez, Alex and Mendelson, Avi and Navarro, Nacho and Cristal, Adrian and Unsal, Osman S.},
	urldate = {2021-09-23},
	date = {2011-10},
}

@inproceedings{romanescu_unified_2010,
	title = {{UNified} Instruction/Translation/Data ({UNITD}) coherence: One protocol to rule them all},
	doi = {10.1109/HPCA.2010.5416643},
	shorttitle = {{UNified} Instruction/Translation/Data ({UNITD}) coherence},
	abstract = {We propose {UNITD}, a unified hardware coherence framework that integrates translation coherence into the existing cache coherence protocol. In {UNITD} coherence protocols, the {TLBs} participate in the cache coherence protocol just like the instruction and data caches, without requiring any changes to the existing coherence protocol. {UNITD} eliminates the need for the software {TLB} shootdown routine, a procedure known to be performance costly and non-scalable. We evaluate snooping and directory {UNITD} coherence protocols on multicore processors with 2-16 cores, and we demonstrate that {UNITD} reduces the performance penalty associated with {TLB} coherence to almost zero.},
	eventtitle = {{HPCA} - 16 2010 The Sixteenth International Symposium on High-Performance Computer Architecture},
	pages = {1--12},
	booktitle = {{HPCA} - 16 2010 The Sixteenth International Symposium on High-Performance Computer Architecture},
	author = {Romanescu, Bogdan F. and Lebeck, Alvin R. and Sorin, Daniel J. and Bracy, Anne},
	date = {2010-01},
	note = {{ISSN}: 2378-203X},
	keywords = {Computer architecture, Hardware, Local activities, Memory management, Microarchitecture, Multicore processing, Multiprocessing systems, Protocols, Software maintenance, Software performance},
}

@inproceedings{gugale_attc_2020,
	location = {New York, {NY}, {USA}},
	title = {{ATTC} (@C): Addressable-{TLB} based Translation Coherence},
	isbn = {978-1-4503-8075-1},
	url = {https://doi.org/10.1145/3410463.3414653},
	doi = {10.1145/3410463.3414653},
	series = {{PACT} '20},
	shorttitle = {{ATTC} (@C)},
	abstract = {Heterogeneous memory systems are getting popular, however they face significant challenges from translation coherence overheads from page remappings. Translation coherence, which is typically implemented in software, can consume up to 50\% of the runtime for some applications in virtualized platforms. In this paper, we propose {ATTC} -- Addressable {TLB}-based Translation Coherence, a hardware translation coherence scheme which eliminates almost all of the overheads associated with software-based coherence mechanisms, and overcomes the challenges in existing hardware schemes. Unlike other proposals ({HATRIC}, {UNITD}) that require on-chip {TLB} tags to enforce coherence and are capable of tracking only the last level page table entries of either the guest or host page tables, {ATTC} tracks changes to both guest and host page tables without requiring any additional metadata in L1, L2 {TLBs}. {ATTC} enforces a "point of coherence'' uniformly for both guest and host page table updates using an addressable {TLB} ({ATLB}) in the {DRAM} akin to the one in [41]. An inverse mapping table ({INVTBL} - present in {DRAM}) that maps host physical pages to {ATLB} locations helps to precisely track translations. We study the proposed {ATTC} scheme in detail for an emerging hybrid memory organization (a mix of {DRAM} and {NVM}) and show that {ATTC} practically eliminates all translation coherence overheads, yielding an average improvement of 35.7\% over a baseline software coherence scheme in virtualized environment and 7.4\% over the hardware {HATRIC} scheme.},
	pages = {481--492},
	booktitle = {Proceedings of the {ACM} International Conference on Parallel Architectures and Compilation Techniques},
	publisher = {Association for Computing Machinery},
	author = {Gugale, Harsh and Gulur, Nagendra and Marathe, Yashwant and John, Lizy K.},
	urldate = {2021-09-23},
	date = {2020-09-30},
	keywords = {hybrid memory, tlb shootdown, translation coherence, virtualization},
}

@article{yan_hardware_2018,
	title = {Hardware Translation Coherence for Virtualized Systems},
	volume = {52},
	issn = {0163-5980},
	url = {https://doi.org/10.1145/3273982.3273988},
	doi = {10.1145/3273982.3273988},
	abstract = {To improve system performance, operating systems ({OSes}) often undertake activities that require modification of virtual-to-physical address translations. For example, the {OS} may migrate data between physical pages to manage heterogeneous memory devices. We refer to such activities as page remappings. Unfortunately, page remappings are expensive. We show that a big part of this cost arises from address translation coherence, particularly on systems employing virtualization. In response, we propose hardware translation invalidation and coherence or {HATRIC}, a readily implementable hardware mechanism to piggyback translation coherence atop existing cache coherence protocols. We perform detailed studies using {KVM}-based virtualization, showing that {HATRIC} achieves up to 30\% performance and 10\% energy benefits, for per-{CPU} area overheads of 0.2\%. We also quantify {HATRIC}'s benefits on systems running Xen and find up to 33\% performance improvements.},
	pages = {57--70},
	number = {1},
	journaltitle = {{ACM} {SIGOPS} Operating Systems Review},
	shortjournal = {{SIGOPS} Oper. Syst. Rev.},
	author = {Yan, Zi and Veselý, Ján and Cox, Guilherme and Bhattacharjee, Abhishek},
	urldate = {2021-09-23},
	date = {2018-08-28},
	keywords = {Virtualization, heterogeneous memory, translation coherence},
}

@inproceedings{kumar_latr_2018,
	location = {New York, {NY}, {USA}},
	title = {{LATR}: Lazy Translation Coherence},
	isbn = {9781450349116},
	url = {https://doi.org/10.1145/3173162.3173198},
	doi = {10.1145/3173162.3173198},
	series = {{ASPLOS} '18},
	shorttitle = {{LATR}},
	abstract = {We propose {LATR}-lazy {TLB} coherence-a software-based {TLB} shootdown mechanism that can alleviate the overhead of the synchronous {TLB} shootdown mechanism in existing operating systems. By handling the {TLB} coherence in a lazy fashion, {LATR} can avoid expensive {IPIs} which are required for delivering a shootdown signal to remote cores, and the performance overhead of associated interrupt handlers. Therefore, virtual memory operations, such as free and page migration operations, can benefit significantly from {LATR}'s mechanism. For example, {LATR} improves the latency of munmap() by 70.8\% on a 2-socket machine, a widely used configuration in modern data centers. Real-world, performance-critical applications such as web servers can also benefit from {LATR}: without any application-level changes, {LATR} improves Apache by 59.9\% compared to Linux, and by 37.9\% compared to {ABIS}, a highly optimized, state-of-the-art {TLB} coherence technique.},
	pages = {651--664},
	booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {Association for Computing Machinery},
	author = {Kumar, Mohan Kumar and Maass, Steffen and Kashyap, Sanidhya and Veselý, Ján and Yan, Zi and Kim, Taesoo and Bhattacharjee, Abhishek and Krishna, Tushar},
	urldate = {2021-09-23},
	date = {2018-03-19},
	keywords = {{TLB}, asynchrony, translation coherence},
}

@inproceedings{awad_avoiding_2017,
	location = {Portland, {OR}},
	title = {Avoiding {TLB} Shootdowns Through Self-Invalidating {TLB} Entries},
	isbn = {978-1-5090-6764-0},
	url = {http://ieeexplore.ieee.org/document/8091251/},
	doi = {10.1109/PACT.2017.38},
	abstract = {Updates to a process's page table entry ({PTE}) renders any existing copies of that {PTE} in any of a system's {TLBs} stale. To prevent a process from making illegal memory accesses using stale {TLB} entries, the operating system ({OS}) performs a costly {TLB} shootdown operation. Rather than explicitly issuing shootdowns, we propose a coordinated {TLB} and page table management mechanism where an expirationtime is associated with each {TLB} entry. An expired {TLB} entry is treated as invalid. For each {PTE}, the {OS} then tracks the latest expiration time of any {TLB} entry potentially caching that {PTE}. No shootdown is issued if the {OS} modifies a {PTE} when its corresponding latest expiration time has already passed.In this paper, we explain the hardware and {OS} support required to support Self-invalidating {TLB} entries ({SITE}). As an emerging use case that needs fast {TLB} shootdowns, we consider memory systems consisting of different types of memory (e.g., faster {DRAM} and slower non-volatile memory) where aggressive migrations are desirable to keep frequently accessed pages in faster memory, but pages cannot migratetoo often because each migration requires a {PTE} update and corresponding {TLB} shootdown. We demonstrate that such heterogeneous memory systems augmented with {SITE} can allow an average performance improvement of 45.5\% over a similar system with traditional {TLB} shootdowns by avoiding more than 65\% of the shootdowns.},
	eventtitle = {2017 26th International Conference on Parallel Architectures and Compilation Techniques ({PACT})},
	pages = {273--287},
	booktitle = {2017 26th International Conference on Parallel Architectures and Compilation Techniques ({PACT})},
	publisher = {{IEEE}},
	author = {Awad, Amro and Basu, Arkaprava and Blagodurov, Sergey and Solihin, Yan and Loh, Gabriel H.},
	urldate = {2021-09-23},
	date = {2017-09},
}

@inproceedings{mazumdar_dead_2021,
	location = {Seoul, Korea (South)},
	title = {Dead Page and Dead Block Predictors: Cleaning {TLBs} and Caches Together},
	isbn = {978-1-66542-235-2},
	url = {https://ieeexplore.ieee.org/document/9407144/},
	doi = {10.1109/HPCA51647.2021.00050},
	shorttitle = {Dead Page and Dead Block Predictors},
	abstract = {The last level {TLB} ({LLT}) and the last level cache ({LLC}) play a critical role in the overall performance of memory-intensive applications. While management of {LLC} content has received significant attention, the same may not be true for {LLT}. In this work, we first explore the well-known concept of dead blocks in caches for {TLBs}. We find that dead pages are fairly common in the {LLT}. Different from dead blocks in {LLCs}, dead pages in {LLTs} are most often dead-on-arrival, i.e., they produce zero hits in the {TLB}. We design a storage-efficient dead page predictor that works with a fraction of storage compared to typical dead block predictors. This is important since an {LLT} itself requires only a few {KBs} of storage compared to {MBs} in {LLC}. We then leverage the dead page information to guide a simple dead block predictor in {LLC}. This is driven by the observation that dead blocks are often concentrated within dead pages. In effect, we designed a dead page predictor and a correlating dead block predictor with a total storage overhead of only 11KB to bypass predicted dead pages and dead blocks in {LLTs} and {LLCs}, respectively. Together, these predictors help improve the {IPC} of a set of 14 memory-intensive workloads by 8.3\%, on average.},
	eventtitle = {2021 {IEEE} International Symposium on High-Performance Computer Architecture ({HPCA})},
	pages = {507--519},
	booktitle = {2021 {IEEE} International Symposium on High-Performance Computer Architecture ({HPCA})},
	publisher = {{IEEE}},
	author = {Mazumdar, Chandrashis and Mitra, Prachatos and Basu, Arkaprava},
	urldate = {2021-09-23},
	date = {2021-02},
}

@inproceedings{amit_optimizing_2017-1,
	location = {{USA}},
	title = {Optimizing the {TLB} shootdown algorithm with page access tracking},
	isbn = {978-1-931971-38-6},
	series = {{USENIX} {ATC} '17},
	abstract = {The operating system is tasked with maintaining the coherency of per-core {TLBs}, necessitating costly synchronization operations, notably to invalidate stale mappings. As core-counts increase, the overhead of {TLB} synchronization likewise increases and hinders scalability, whereas existing software optimizations that attempt to alleviate the problem (like batching) are lacking. We address this problem by revising the {TLB} synchronization subsystem. We introduce several techniques that detect cases whereby soon-tobe invalidated mappings are cached by only one {TLB} or not cached at all, allowing us to entirely avoid the cost of synchronization. In contrast to existing optimizations, our approach leverages hardware page access tracking. We implement our techniques in Linux and find that they reduce the number of {TLB} invalidations by up to 98\% on average and thus improve performance by up to 78\%. Evaluations show that while our techniques may introduce overheads of up to 9\% when memory mappings are never removed, these overheads can be avoided by simple hardware enhancements.},
	pages = {27--39},
	booktitle = {Proceedings of the 2017 {USENIX} Conference on Usenix Annual Technical Conference},
	publisher = {{USENIX} Association},
	author = {Amit, Nadav},
	urldate = {2021-09-23},
	date = {2017-07-12},
}

@article{lustig_tlb_2013,
	title = {{TLB} Improvements for Chip Multiprocessors: Inter-Core Cooperative Prefetchers and Shared Last-Level {TLBs}},
	volume = {10},
	issn = {1544-3566},
	url = {https://doi.org/10.1145/2445572.2445574},
	doi = {10.1145/2445572.2445574},
	shorttitle = {{TLB} Improvements for Chip Multiprocessors},
	abstract = {Translation Lookaside Buffers ({TLBs}) are critical to overall system performance. Much past research has addressed uniprocessor {TLBs}, lowering access times and miss rates. However, as Chip {MultiProcessors} ({CMPs}) become ubiquitous, {TLB} design and performance must be reevaluated. Our article begins by performing a thorough {TLB} performance evaluation of sequential and parallel benchmarks running on a real-world, modern {CMP} system using hardware performance counters. This analysis demonstrates the need for further improvement of {TLB} hit rates for both classes of application, and it also points out that the data {TLB} has a significantly higher miss rate than the instruction {TLB} in both cases. In response to the characterization data, we propose and evaluate both Inter-Core Cooperative ({ICC}) {TLB} prefetchers and Shared Last-Level ({SLL}) {TLBs} as alternatives to the commercial norm of private, per-core L2 {TLBs}. {ICC} prefetchers eliminate 19\% to 90\% of Data {TLB} (D-{TLB}) misses across parallel workloads while requiring only modest changes in hardware. {SLL} {TLBs} eliminate 7\% to 79\% of D-{TLB} misses for parallel workloads and 35\% to 95\% of D-{TLB} misses for multiprogrammed sequential workloads. This corresponds to 27\% and 21\% increases in hit rates as compared to private, per-core L2 {TLBs}, respectively, and is achieved this using even more modest hardware requirements. Because of their benefits for parallel applications, their applicability to sequential workloads, and their readily implementable hardware, {SLL} {TLBs} and {ICC} {TLB} prefetchers hold great promise for {CMPs}.},
	pages = {2:1--2:38},
	number = {1},
	journaltitle = {{ACM} Transactions on Architecture and Code Optimization},
	shortjournal = {{ACM} Trans. Archit. Code Optim.},
	author = {Lustig, Daniel and Bhattacharjee, Abhishek and Martonosi, Margaret},
	urldate = {2021-09-23},
	date = {2013-04-01},
	keywords = {{TLB} prefetching, Translation lookaside buffer, performance evaluation, shared last-level {TLB}, simulation},
}

@inproceedings{amit_dont_2020,
	location = {New York, {NY}, {USA}},
	title = {Don't shoot down {TLB} shootdowns!},
	isbn = {978-1-4503-6882-7},
	url = {https://doi.org/10.1145/3342195.3387518},
	doi = {10.1145/3342195.3387518},
	series = {{EuroSys} '20},
	abstract = {Translation Lookaside Buffers ({TLBs}) are critical for building performant virtual memory systems. Because most processors do not provide coherence for {TLB} mappings, {TLB} shootdowns provide a software mechanism that invokes inter-processor interrupts ({IPLs}) to synchronize {TLBs}. {TLB} shootdowns are expensive, so recent work has aimed to avoid the frequency of shootdowns through techniques such as batching. We show that aggressive batching can cause correctness issues and addressing them can obviate the benefits of batching. Instead, our work takes a different approach which focuses on both improving the performance of {TLB} shootdowns and carefully selecting where to avoid shootdowns. We introduce four general techniques to improve shootdown performance: (1) concurrently flush initiator and remote {TLBs}, (2) early acknowledgement from remote cores, (3) cacheline consolidation of kernel data structures to reduce cacheline contention, and (4) in-context flushing of userspace entries to address the overheads introduced by Spectre and Meltdown mitigations. We also identify that {TLB} flushing can be avoiding when handling copy-on-write ({CoW}) faults and some {TLB} shootdowns can be batched in certain system calls. Overall, we show that our approach results in significant speedups without sacrificing safety and correctness in both microbenchmarks and real-world applications.},
	pages = {1--14},
	booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
	publisher = {Association for Computing Machinery},
	author = {Amit, Nadav and Tai, Amy and Wei, Michael},
	urldate = {2021-09-23},
	date = {2020-04-15},
}